<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  lxp121's BLOG on CUDA and DL
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="lxp121's BLOG on CUDA and DL" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:china.xipengli.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; lxp121's BLOG on CUDA and DL</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="dl.html">Deep Learning</a></li>
        
            <li><a href="cuda.html">CUDA</a></li>
        
            <li><a href="trt.html">TensorRT</a></li>
        
            <li><a href="Git.html">Git</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="20180121_CUDA_Volatile_And_Memory_Fence.html">
                
                  <h1>Volatile 限定词（Qualifier）和内存围栏（memory fence)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>这边blog是从nvidia blog上最新的文章<a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a> 引申而来。原文主要介绍了CUDA 9.0中引入的一些新的warp级别的原语(primitives)。本文中就里面提到一些概念进行引申讲解。</p>

<p>在讲到__syncwarp()时候提到了：</p>

<blockquote>
<p>The __syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed a __syncwarp() (with the same mask) before resuming execution. <strong>It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</strong></p>
</blockquote>

<p>那这个<strong>memory fence</strong>是一个什么概念呢？</p>

<h2 id="toc_0">内存围栏（memory fence)</h2>

<p>CUDA 编程模型假定了一种弱顺序(weakly-ordered)内存模型。也就是说一个CUDA thread 将数据写入共享内存(shared memory)，全局内存（global memory)，pinned CPU内存（page-locked host memory)，或者对等设备（peer device，可以与当前设备进行P2P通信的设备）的内存时，并不需要与数据被观察到的顺序一致。</p>

<p>用一个实际例子来说明：<br/>
假设下面的代码中，thread 1 执行writeXY()，而thread 2 执行readXY()，thread 2读取到的值是多少呢？</p>

<pre><code class="language-cpp">__device__ volatile int X = 1, Y = 2; 

__device__ void writeXY() { 
    X = 10; Y = 20; 
} 

__device__ void readXY() { 
    int A = X; int B = Y; 
}
</code></pre>

<p>如果是一个<strong>强顺序内存模型(strongly-ordered memory model)</strong>中，只有可能是</p>

<ul>
<li>A=1, B=2</li>
<li>A=10, B=2</li>
<li>A=10, B=20</li>
</ul>

<p>因为虽然两个threads的执行完成可能有先有后，但是A=X, B=Y的必须在语义学上是前后关系（也就是被观察顺序）。因此，上述三种情况分别是：</p>

<ul>
<li>A=1, B=2：thread 1 没有执行完X=10, Y=20的操作；thread 2 已经执行完了A=X, B=Y的操作。</li>
<li>A=10, B=2：thread 2 执行完X=10, 还没有执行Y=20；thread 2 已经执行完了A=X, B=Y的操作，所以Y的值还没有更新。</li>
<li>A=10, B=20：thread 1 执行完X=10, Y=20；thread 2读取了对应数据，X和Y都是最新的值。</li>
</ul>

<p>但是在CUDA使用的<strong>弱顺序内存模型</strong>中，可能会出现：</p>

<ul>
<li>A=1, B=20</li>
</ul>

<p>的情况。也就是说，虽然语义上A=10比B=20更早出现，但是CUDA的内存模型并不保证这一顺序。</p>

<p><strong>内存围栏函数可以被用来保证实际内存读取顺序和语义学顺序相同。</strong><br/>
在CUDA中提供了三个内存围栏函数：</p>

<pre><code class="language-cpp">void __threadfence_block();
void __threadfence();
void __threadfence_system();
</code></pre>

<p>这些函数都保证在函数调用前的内存操作不会晚于函数调用后的。</p>

<p>我们考虑一个比较常见的情景，这个技巧会在很多地方用到，大家也可以自己思考一下对这个问题有一些什么样的解决方案：<br/>
用一次核函数（kernel）调用计算N个数的和，也就是我们常说的reduction操作。我们实现的逻辑是：</p>

<ul>
<li>每个block首先计算这个数组的一个子集，并将计算结果写到全局内存中。提示，在改部分计算中可以使用warp原语、共享内存等方法来进一步加速，请参考相应的post。</li>
<li>当所有block都完成了这个计算，最后一个block从全局内存中读取这些部分和，并且将他们相加得到最终结果。</li>
<li>为了确认哪一个block最后完成操作。每一个block都会用原子加法(atomic add)将一个计数器（counter）加一。这样，最后一个block就是读取到counter值为gridDim.x-1的那个block。</li>
</ul>

<p>如果没有在存储部分和与计数器加一之间插入内存未来，计数器可能会在部分和存储完成之前加一，这样最终计算的结果可能会缺失一部分。</p>

<pre><code class="language-cpp">__device__ unsigned int count = 0; 
__shared__ bool isLastBlockDone; 

__global__ void sum(const float* array, unsigned int N, volatile float* result) 
{ 
    // Each block sums a subset of the input array. 
    float partialSum = calculatePartialSum(array, N); 
    
    if (threadIdx.x == 0) { 
        // Thread 0 of each block stores the partial sum 
        // to global memory. The compiler will use 
        // a store operation that bypasses the L1 cache 
        // since the &quot;result&quot; variable is declared as 
        // volatile. This ensures that the threads of 
        // the last block will read the correct partial 
        // sums computed by all other blocks. 
        result[blockIdx.x] = partialSum; 
        
        // Thread 0 makes sure that the incrementation 
        // of the &quot;count&quot; variable is only performed after 
        // the partial sum has been written to global memory. 
        __threadfence(); 
        
        // Thread 0 signals that it is done. 
        unsigned int value = atomicInc(&amp;count, gridDim.x); 
        
        // Thread 0 determines if its block is the last 
        // block to be done. 
        isLastBlockDone = (value == (gridDim.x - 1)); } 
        
        // Synchronize to make sure that each thread reads 
        // the correct value of isLastBlockDone. 
        __syncthreads(); 
        
        if (isLastBlockDone) { 
            // The last block sums the partial sums 
            // stored in result[0 .. gridDim.x-1] 
            float totalSum = calculateTotalSum(result); 
            
            if (threadIdx.x == 0) { 
                // Thread 0 of last block stores the total sum 
                // to global memory and resets the count 
                // varialble, so that the next kernel call 
                // works properly. 
                result[0] = totalSum; count = 0; 
            } 
        } 
    }
</code></pre>

<p>内存围栏只会影响一个线程中内存操作的顺序。他并不能像__syncthreads()那样保证这些内存操作对于同block中的其他线程可见。也就是说，由于有L1 cache的存在，其他block可能读取到的是没有更新的result值。在我们以往的经验中，这个是用__syncthreads()来保证的。在这个例子里，volatile限定词被用来保证内存操作的可见性。</p>

<h2 id="toc_1">volatile 限定词（Qualifier）</h2>

<p>从<a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#volatile-qualifier">CUDA编程手册</a>我们可以看到：</p>

<blockquote>
<p>The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions (Memory Fence Functions) and memory visibility semantics of synchronization functions (Synchronization Functions).</p>

<p>These optimizations can be disabled using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction.</p>
</blockquote>

<p>对于nvcc编译器来说，编译器是可以自由地优化全局内存(global memory)和共享内存(shared memory)的读写顺序。例如，这种优化可以是通过将共享内存的读取缓存到寄存器或者L1缓存中来实现的。但是，这种优化，必须要服从由内存围栏(memory fence)函数所定义出的内存顺序语义(memory ordering semantics)，还必须要服从同步函数(synchronization function)所限制的内存可见性语义（memory visibility semantics)。</p>

<p>如果我们对某一个内存（全局内存或共享内存）使用了volatile这个关键词，<strong>编译器将认为该内存的值允许在任意时刻被其他thread修改</strong>。因此，该变量的所有引用，都会被编译为真实的读写指令。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='cuda.html'>CUDA</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180121_TF_to_TRT.html">
                
                  <h1>TensorFlow模型的保存，已经TensoRT中tensorFlow 模型的导入（草稿）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	
                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='dl.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180116_DL_Tuning_Reprint.html">
                
                  <h1>深度学习中的优化调参细节（转帖）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">深度学习中的技巧</h2>

<ul>
<li>初始化参数尽量小一些，这样 softmax 的回归输出更加接近均匀分布，使得刚开始网络并不确信数据属于哪一类；另一方面从数值优化上看我们希望我们的参数具有一致的方差（一致的数量级），这样我们的梯度下降法下降也会更快。同时为了使每一层的激励值保持一定的方差，我们在初始化参数（不包括偏置项）的方差可以与输入神经元的平方根成反比。</li>
<li>学习率（learning rate）的设置应该随着迭代次数的增加而减小，个人比较喜欢每迭代完一次epoch也就是整个数据过一遍，然后对学习率进行变化，这样能够保证每个样本得到了公平的对待。</li>
<li>滑动平均模型，在训练的过程中不断的对参数求滑动平均这样能够更有效的保持稳定性，使其对当前参数更新不敏感。例如加动量项的随机梯度下降法就是在学习率上应用滑动平均模型。</li>
<li>在验证集上微小的提升未必可信，一个常用的准则是增加了30个以上的正确样本，能够比较确信算法有了一定的提升。 </li>
<li>不要太相信模型开始的学习速度，这与最终的结果基本没有什么关系。一个低的学习速率往往能得到较好的模型。</li>
<li>在深度学习中，常用的防止过拟合的方法除了正则化，dropout和pooling之外，还有提前停止训练的方法——就是看到我们在验证集的上的正确率开始下降就停止训练。</li>
<li>当激活函数是RELU时，我们在初始化偏置项时，为了避免过多的死亡节点（激活值为0）一般可以初始化为一个较小的正值。</li>
<li>基于随机梯度下降的改进优化算法有很多种，在不熟悉调参的情况，建议使用Adam方法</li>
<li>训练过程不仅要观察训练集和测试集的loss是否下降、正确率是否提高，对于参数以及激活值的分布情况也要及时观察，要有一定的波动。</li>
<li>如果我们设计的网络不work，在训练集的正确率也很低的话，我们可以减小样本数量同时去掉正则化项，然后进行调参，如果正确率还是不高的话，就说明我们设计的网络结果可能有问题。</li>
<li>fine-tuning的时候，可以把新加层的学习率调高，重用层的学习率可以设置的相对较低。</li>
<li>在隐藏层的激活函数，tanh往往比sigmoid表现更好。</li>
<li>针对梯度爆炸的情况我们可以使用梯度截断来解决，尤其在RNN中由于存在相同的循环结构，导致相同参数矩阵的连乘，更加容易产生梯度爆炸。当然，使用LSTM和GRU等更加优化的模型往往是更好地选择。</li>
<li>正则化输入，也就是让特征都保持在0均值和1方差。（注意做特征变换时请保持训练集合测试集进行了相同的变化）</li>
<li>梯度检验：当我们的算法在训练出现问题而进行debug时，可以考虑使用近似的数值梯度和计算的梯度作比较检验梯度是否计算正确。</li>
<li>搜索超参数时针对经典的网格搜索方法，这里有两点可以改善的地方：1）不用网格，用随机值，因为这样我们一次实验参数覆盖范围更广，尤其在参数对结果影响级别相差很大的情况下。2）不同数量级的搜索密度是不一样的，不能均分。</li>
</ul>

<h2 id="toc_1">CNN中的独特技巧</h2>

<ul>
<li>CNN中将一个大尺寸的卷积核可以分解为多层的小尺寸卷积核或者分成多层的一维卷积。这样能够减少参数增加非线性。</li>
<li>CNN中的网络设计应该是逐渐减小图像尺寸，同时增加通道数，让空间信息转化为高阶抽象的特征信息。</li>
<li>CNN中可以利用Inception方法来提取不同抽象程度的高阶特征，用ResNet的思想来加深网络的层数。</li>
<li>CNN处理图像时，常常会对原图进行旋转、裁剪、亮度、色度、饱和度等变化以增大数据集增加鲁棒性。</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/16</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='dl.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180201_TRT_Plugin_API.html">
                
                  <h1>TensorRT Plugin APIs 介绍（草稿）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>TensorRT 中一个非常重要的特性是对于Plugin的支持。目前来说，标准Caffe中的CNN相关神经网络层已经完全支持。但是有很多改动版本的Caffe里面新添加的网络层是不支持的。例如SSD网络里面的priorbox, flatten, permute, detecion_out等。这些网络层在tensorRT里面可以使用Plugin的方式来支持。<br/>
因为整体上说，tensorRT的优化主要针对CBR (convolution-bias-relu)的合并，卷积的优化计算，低精度（int8，fp16）也主要是针对卷积层进行的。因此少量的plugin不会对性能产生较大影响。<br/>
但是对于YOLOv2这个网络，使用了目前tensorRT里面还不直接支持的PReLU, 如果把这个层使用plugin来实现，会出现CBR过程被打断，一些优化会受到影响，整体性能提高幅度可能不大。</p>

<p>本文先简要介绍一下定义一个plugin需要重载的IPlugin里面的函数。</p>

<p>总体上来说，TensorRT运行分为若干个步骤：</p>

<ol>
<li>创建网络(create network)，这一步可以是通过caffe 的parser，也就是以caffe的prototxt和caffemodel作为输入。nvCafferParser 回去分析这个网络结构，并且解析出其中各个网络层，如果是tensorRT已经支持的网络层，则会调用相应的tensorRT的API来添加到tensorRT的network中。如果是没有的网络层，但是已经在PluginFactory工厂里面定义了（通过isPlugin(layerName)来判断），则会标记为plugin添加到network中。</li>
<li>创建TensorRT引擎(create TensorRT engine)，这一步是tensorRT回去根据配置生成优化engine。主要的优化项目包括：合并网络层（例如CBR的合并，多个相同大小的CBR进行进一步合并），消除多余的网络层（例如concat层），根据提供的最大Batch数(maxium batch size)，workspace来选择相应的算法和CUDA实现(kernel)，预分配中间变量、输入输出buffer等。</li>
<li>运行时（runtime), 在这个阶段，</li>
</ol>

<h2 id="toc_0">When creating the network (from parser or API)</h2>

<ul>
<li>getNbOutputs()</li>
<li>getOutputDimensions()</li>
</ul>

<h2 id="toc_1">By the builder</h2>

<ul>
<li>configure()</li>
<li>getWorkSpaceSize()</li>
</ul>

<h2 id="toc_2">At runtime</h2>

<ul>
<li>initialize() when the engine context is constructed</li>
<li>enqueue() at inference time</li>
<li>terminate() when the engine context is destroyed</li>
</ul>

<h2 id="toc_3">For serialization</h2>

<ul>
<li>getSerializationSize()</li>
<li>serialize()</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='trt.html'>TensorRT</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15112406136583.html">
                
                  <h1>RNNs with equations</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>实际上使用公式来理解RNN更加简单，例如LSTM实际上就是求解了三个门i (input), o(output), f(forget), 三个门就是三个全连接层+激活层。而所谓的cell，就是计算输入的信息（本时刻，和上个时刻），至于这些信息是否保留，是两个门forget gate 和input gate来保证的。</p>

<h2 id="toc_0">LSTM</h2>

<ul>
<li>f: forget gate, 遗忘门，上一时刻的\(C_{t-1}\)的权重</li>
<li>i: input gate, 输入门，当前计算的\(C_{t}\)的权重</li>
<li>o: output gate, 输出门，当前计算的\(C_{t}\)到hidden output的权重。</li>
</ul>

<p>\[ f_t = \sigma(W_f\cdot [h_{t-1},x_t]^T+b_f)\]<br/>
\[ i_t = \sigma(W_i\cdot [h_{t-1},x_t]^T+b_i)\]<br/>
\[ \tilde{C}_t = \tanh (W_c\cdot [h_{t-1},x_t]^T+b_c)\]<br/>
\[ o_t = \sigma(W_o\cdot [h_{t-1},x_t]^T+b_o)\]<br/>
\[ C_t = f_t C_{t-1} + i_t  \tilde{C}_t \]<br/>
\[ h_t = o_t \tanh(C_t) \]</p>

<h2 id="toc_1">LSTM with peephole</h2>

<p>和LSTM相比，在计算各个gate的时候，顺便参考了一下\(C_{t-1}\)(input, forget)或者\(C_t\)(output)</p>

<p>\[ f_t = \sigma(W_f\cdot [C_{t-1},h_{t-1},x_t]^T+b_f)\]<br/>
\[ i_t = \sigma(W_i\cdot [C_{t-1},h_{t-1},x_t]^T+b_i)\]<br/>
\[ \tilde{C}_t = \tanh (W_c\cdot [h_{t-1},x_t]^T+b_c)\]<br/>
\[ C_t = f_t C_{t-1} + i_t  \tilde{C}_t \]<br/>
\[ o_t = \sigma(W_o\cdot [C_t,h_{t-1},x_t]^T+b_o)\]<br/>
\[ h_t = o_t \tanh(C_t) \]</p>

<h2 id="toc_2">LSTMP</h2>

<p>和LSTM相比，在计算最终的hidden output的时候，多添加了一个\(W_r\)<br/>
\[ f_t = \sigma(W_f\cdot [h_{t-1},x_t]^T+b_f)\]<br/>
\[ i_t = \sigma(W_i\cdot [h_{t-1},x_t]^T+b_i)\]<br/>
\[ \tilde{C}_t = \tanh (W_c\cdot [h_{t-1},x_t]^T+b_c)\]<br/>
\[ o_t = \sigma(W_o\cdot [h_{t-1},x_t]^T+b_o)\]<br/>
\[ C_t = f_t C_{t-1} + i_t  \tilde{C}_t \]<br/>
\[ h_t = W_r (o_t \tanh(C_t)) \]</p>

<h2 id="toc_3">GRU</h2>

<p>GRU相比LSTM更加简单，它只有两个门(z,r), 并且不需要单独的cell单元来保存状态。<br/>
\[ z_t = \sigma(W_z\cdot [h_{t-1}, x_t]^T)\]<br/>
\[ r_t = \sigma(W_r\cdot [h_{t-1}, x_t]^T)\]<br/>
\[ \tilde{h}_t = \tanh(W\cdot [r_t\times h_{t-1}, x_t]^T)\]<br/>
\[ h_t = (1-z_t)h_{t-1} + z_t \tilde{h}_t\]</p>

<h2 id="toc_4">关于Bias在不同框架中的处理</h2>

<p>不同框架中，对于b有不同的处理，在tf和cudnn中，认为每个门的计算是分开的，因此有两个bias，即，<br/>
\[ f_t = \sigma (W_{f1} h_{t-1} + b_{f1} + W_{f1} x_t + b_{f2}) \]<br/>
因此对于tf/cudnn，weight的数目是：</p>

<pre><code class="language-cpp">cnt_w = 4* ( (intput_dim + hidden_dim) * hidden_dim ) 
        + (nLayeer - 1) * 4 * ( 2 * hidden_dim * hidden_dim);
cnt_b = 8 * hidden_dim * nLayer     
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2017/11/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='dl.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="media/15161603048345/Blog_LOGO.png" /></div>
            
                <h1>lxp121's BLOG on CUDA and DL</h1>
                <div class="site-des"></div>
                <div class="social">



<a target="_blank" class="linkedin" href="https://www.linkedin.com/in/xipengli" title="LinkedIn">LinkedIn</a>





<a target="_blank" class="github" target="_blank" href="https://github.com/lxp121" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lxp121@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="dl.html"><strong>Deep Learning</strong></a>
        
            <a href="cuda.html"><strong>CUDA</strong></a>
        
            <a href="trt.html"><strong>TensorRT</strong></a>
        
            <a href="Git.html"><strong>Git</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15231593087199.html">TensorFlow-TensorRT 的使用及其限制（草稿）</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15203065717420.html">GPU代码的向后兼容问题（草稿）</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180212_git_gitignore.html">Git 修改.gitignore不起作用</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180212_git_move_commits_to_new_branch.html">Git 将最近改动移到新的分支(branch)上</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180125_CUDA_Warp_Level_Primitives.html">使用 CUDA 的 warp-level 原语</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
