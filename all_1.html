<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  lxp121's BLOG on CUDA and DL
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="lxp121's BLOG on CUDA and DL" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:china.xipengli.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; lxp121's BLOG on CUDA and DL</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about.html">About</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="dl.html">Deep Learning</a></li>
        
            <li><a href="cuda.html">CUDA</a></li>
        
            <li><a href="trt.html">TensorRT</a></li>
        
            <li><a href="Git.html">Git</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="20180125_CUDA_Warp_Level_Primitives.html">
                
                  <h1>使用 CUDA 的 warp-level 原语</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>本文是NVIDA Blog文章<a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a>的全文翻译和深度解析。</p>

<p>从CUDA 9.0 开始，CUDA引入了更加灵活的group的选择，这一方面使得CUDA编程更加简单，一方面也使得一些原有的功能发生了一些改变。本文重点对warp级别的原语（primitives）进行一些介绍。</p>

<p>在GPU中，线程（thread）被组织为warp，然后warp会按照SIMT（单指令多线程，Single Instruction Multiple Thread）的形式来执行。很多CUDA程序都可以通过充分利用warp执行来达到很高的性能。</p>

<h2 id="toc_0">Warp级别原语（Warp-level Primitives）</h2>

<p>NVIDIA GPUs和CUDA编程模型使用一种被称为SIMT的执行模式。SIMT和SIMD（Single Instruction, Multiple Data)的区别主要是：</p>

<ul>
<li>在SIMD架构中，每个指令在不同的数据上并行的进行相同的操作。SIMD通常采用向量寄存器和向量执行单元来实现。而vector执行是通过一个标量的向量来实现的。简言之，一个线程调度，多个向量寄存器和执行单元上并行完成相同指令。</li>
<li>而SIMT架构中，并不只是使用单一的线程，并且数据也不要求采用向量存储方式。多个线程会发起普通的指令（而非向量指令）在任意数据上执行。我们可以想象一下GPU的执行时候，可以根据每个thread的线程来计算需要操作的数据位置。实际上，可以在SIMT的架构中进一步实现SIMD的子架构，以Pascal架构中支持的INT8计算模式为例。该模式可以非常高效地提高深度神经网络的计算。在INT8的计算中，要求被计算的4个数据相互邻近（每个8bit），4个指令在一个GPU线程上执行。而这个线程是SIMT中的一个线程。</li>
</ul>

<p>在NVIDIA的GPU上，32个并行线程（相邻线程）被组成一个warp，每一个线程可以访问它自己的寄存器，从不同显存地址（可以不相邻）读写数据。并且，这些线程是可以支持分歧的控制流路径（divergent control flow paths)。本文中，我们重点放在使用Warp级别原语的使用。</p>

<p><em><a href="#code1">代码1</a></em> 是一个warp-level 原语的例子。我们使用了一个<strong>__shfl_down_sync()</strong> 来实现一个tree-reduction方式来计算一个warp中线程val值得和。该代码执行完之后，warp中的第一个线程中的val值就等于最终的和。</p>

<p><strong><div align = lift><span id="code1">代码1: warp-level reduction</span></div></strong> </p>

<pre class="line-numbers"><code class="language-cpp">#define FULL_MASK 0xffffffff
for (int offset = 16; offset &gt; 0; offset /= 2)
    val += __shfl_down_sync(FULL_MASK, val, offset);
</code></pre>

<p>这个计算可以用下面的图来说明：<br/>
<img src="media/15168532667210/15171231191469.png" alt="使用__shfl_down_sync()来实现warp级别的并行reduction"/><br/>
<strong><div align = center><span id="image1">图片1：使用__shfl_down_sync()来实现warp级别的并行reduction</span></div></strong></p>

<p>一个warp中包含32通道（lane），每一个线程占用一个通道。对于处于 X 通道的线程，<strong>__shfl_down_sync(FULL_MASK, val, offset)</strong> 来获得在同一个warp中的第 <strong>X+offset</strong> 通道的val值。这一数据交换在regitster中进行的，因此其效率高于共享内存。使用共享内存需要一次读，一次写，并且需要一个额外的寄存器来保存这些地址。</p>

<p>CUDA 9中包含了三类warp级别原语：</p>

<ol>
<li>同步地数据交换（Synchronized data exchange）：在同一warp中不同thread之间交换数据：
<ul>
<li>__all_sync, __any_sync, __uni_sync, __ballot_sync</li>
<li>__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</li>
<li>__match_any_sync, __match_all_sync</li>
</ul></li>
<li>Active mask query: 返回一个32-bit的mask来表示warp中哪些thread是活动的。
<ul>
<li>__activemask</li>
</ul></li>
<li>线程同步（Thread synchronization)：同步warp中的线程，并提供一个内存围栏（关于内存围栏的概念，可以在<a href="20180121_CUDA_Volatile_And_Memory_Fence.html">Volatile 限定词（Qualifier）和内存围栏（memory fence) </a>中找到详细解释）。</li>
</ol>

<h2 id="toc_1">同步地数据交换（Synchronized data exchange）</h2>

<p>每一个”同步地数据交换“原语都会在warp中的一组线程（不一定是所有线程）上执行一个聚合操作（collective operation)。<em><a href="#code2">代码2</a></em> 展示了其中的三个操作。每个线程可以调用 <strong>__shfl_sync()</strong> 或者 <strong>__shfl_down_sync()</strong> 来接收同一个warp中其他线程的数据。每个线程也可以调用<strong>__ballot_sync()</strong>来检验一个mask中的所有活动线程是否满足predicate的条件。该原语返回一个整形数，该整形数的第N位(bit)为真，当且仅当：</p>

<ul>
<li>predicate的检测结果为真；</li>
<li>第N个线程为active。</li>
</ul>

<p><strong><div align = lift><span id="code2">代码2: Synchronized data exchange warp-level primitives</span></div></strong> </p>

<pre class="line-numbers"><code class="language-cpp">int __shfl_sync(unsigned mask, int val, int src_line, int width=warpSize);
int __shfl_down_sync(unsigned mask, int var, unsigned detla, 
                     int width=warpSize);
int __ballot_sync(unsigned mask, int predicate);
</code></pre>

<p>通常使用一个32位的掩码（mask）来表示参与原语的线程。每一个参与的线程在执行相应操作之前必须同步，以保证其得到正确的结果。例如<strong>__shfl_sync</strong>实际是先同步所有的数据，再执行shfl的操作。因此<strong>__shfl_sync</strong>实际上可以保证一个同步。这里是一个很重要的变化：<strong>在CUDA9.0 之前，__shfl这个操作实际上是不保证一定同步的，但是在绝大多数的应用中，并没有出现不同步的情况。在CUDA9.0 中，不再使用__shfl等非同步原语。如果你有CUDA9.0之前的代码，使用了__shfl原语，请将其更换为__shfl_sync。</strong></p>

<p>一个常被问到的问题是：应该使用什么样的掩码参数？我们可以简单的理解为mask就是一组需要参与聚合操作的线程的id。所以，这个线程组的组成实际是程序逻辑决定的。我们还是以<em><a href="#code1">代码1</a></em> 这个reduction代码为例子。假设我们希望计算input[]数组所有元素的和，并且这个数组长度NUM_ELEMENTS是小于这个block里面的线程数目的。我们可以使用<em><a href="#code3">代码3</a></em>这个代码来实现。 </p>

<p><strong><div align = lift><span id="code3">代码3: </span></div></strong> </p>

<pre class="line-numbers"><code class="language-cpp">unsigned mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);
if (threadIdx.x &lt; NUM_ELEMENTS) { 
    val = input[threadIdx.x]; 
    for (int offset = 16; offset &gt; 0; offset /= 2)
        val += __shfl_down_sync(mask, val, offset);
    …
}
</code></pre>

<p>这个代码首先使用 <em>threadIdx.x &lt; NUM_ELEMENTS</em> 来确认一个线程是否参与这个reduction过程。其中<em>__ballot_sync()</em> 是用来计算<em>__shfl_down_sync()</em>中使用的掩码的。而<em>__ballot_sync()</em>本身使用FULL_MASK （0xffffffff），表示所有的32个线程都需要参与投票。</p>

<p>在Volta或者更晚一些的架构中，数据交换原语可以被用于线程分歧分支（thread-divergent branchs）中-- 一个线程可以执行和warp中其他线程不同的路径。<em><a href="#code4">代码4</a></em> 展示了这样一个例子，一个warp中所有线程获得第0通道的val值，但是奇数和偶数的线程执行一个if语句中的不同分支。 </p>

<p><strong><div align = lift><span id="code4">代码4: </span></div></strong> </p>

<pre class="line-numbers"><code class="language-cpp">if (threadIdx.x % 2) {
    val += __shfl_sync(FULL_MASK, val, 0);
…
}
else {
    val += __shfl_sync(FULL_MASK, val, 0);
…
}
</code></pre>

<p>在最新的Volta（或者未来的）GPU上，你可以使用warp同步原语，而不需要去考虑这个函数是执行在哪个线程分歧分支上。</p>

<h2 id="toc_2">查询活跃线程的掩码（Active Mask Query）</h2>

<p><strong>__active_mask()</strong> 返回一个32位无符号整形掩码（mask），该掩码的每一位表示了当前warp中线程的活动（active）情况。换句话说，该掩码表示了在同一个warp中也在执行<strong>__active_mask()</strong>的线程。该原语对于opportunistic warp-level programming 技术（后续会详细解释）、debugging和理解程序行为非常有用。</p>

<p>但是，正确的使用<strong>__active_mask()</strong>是非常重要的。<em><a href="#code5">代码5</a></em> 是一个<strong>错误的</strong>使用的例子。该代码试图进行<em><a href="#code4">代码4</a></em> 中的求和reduction，但是它不使用 <strong>__active_mask()</strong> </p>

<p>__activemask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same __activemask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.</p>

<p>However, it’s important to use __active_mask() correctly. Listing 5 illustrates an incorrect use. The code tries to perform the same sum reduction  shown in Listing 4, but instead of using __ballot_sync() to compute the mask before the branch, it uses __active_mask() inside the branch. This is incorrect, as it would result in partial sums instead of a total sum. The CUDA execution model does not guarantee that all threads taking the branch together will execute the __active_mask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>

<p><strong><div align = lift><span id="code5">代码5: </span></div></strong> </p>

<pre class="line-numbers"><code class="language-cpp">//
// Incorrect use of __active_mask()
//
if (threadIdx.x &lt; NUM_ELEMENTS) { 
    unsigned mask = __active_mask(); 
    val = input[threadIdx.x]; 
    for (int offset = 16; offset &gt; 0; offset /= 2)
        val += __shfl_down_sync(mask, val, offset);
    …
}
</code></pre>

<h2 id="toc_3">Warp 同步（Synchronization）</h2>

<p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the __syncwarp() primitive to synchronize threads in a warp. It is similar to the __syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.</p>

<pre class="line-numbers"><code class="language-cpp">void __syncwarp(unsigned mask=FULL_MASK);
</code></pre>

<p>The __syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed a __syncwarp() (with the same mask) before resuming execution. It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</p>

<p>Listing 6 shows an example of shuffling the ownership of matrix elements among threads in a warp.</p>

<pre class="line-numbers"><code class="language-cpp">float val = get_value(…);
__shared__ float smem[4][8];
 
//   0  1  2  3  4  5  6  7 
//   8  9 10 11 12 13 14 15 
//  16 17 18 19 20 21 22 23
//  24 25 26 27 28 29 30 31
int x1 = threadIdx.x % 8;
int y1 = threadIdx.x / 8;
 
//   0  4  8 12 16 20 24 28
//   1  5 10 13 17 21 25 29
//   2  6 11 14 18 22 26 30
//   3  7 12 15 19 23 27 31
int x2= threadIdx.x / 4;
int y2 = threadIdx.x % 4;
 
smem[y1][x1] = val;
__syncwarp();
val = smem[y2][x2];
 
use(val);
</code></pre>

<p>Assume a 1-D thread block is used (i.e. threadIdx.y is always 0). At the beginning of the code, each thread in a warp owns one element of a 4×8 matrix with row-major indexing. In other words, lane 0 owns [0][0] and lane 1 owns [0][1]. Each thread stores its value into the corresponding position of a 4×8 array in shared memory. Then __syncwarp() is used to ensure all threads have done the store, before each thread reads from a transposed position in the array. In the end, each thread in the warp owns one element of the matrix with column-major indexing: lane 0 owns [0][0] and lane 1 owns [1][0].</p>

<p>Make sure that __syncwarp() separates shared memory reads and writes to avoid race conditions. Listing 7 illustrates an incorrect use in a tree sum reduction in shared memory. There is a shared memory read followed by a shared memory write between every two __syncwarp() calls. The CUDA programming model does not guarantee that all the reads will be performed before all the writes, so there is a race condition.</p>

<pre class="line-numbers"><code class="language-cpp">unsigned tid = threadIdx.x;

// Incorrect use of __syncwarp()
shmem[tid] += shmem[tid+16]; __syncwarp();
shmem[tid] += shmem[tid+8];  __syncwarp();
shmem[tid] += shmem[tid+4];  __syncwarp();
shmem[tid] += shmem[tid+2];  __syncwarp();
shmem[tid] += shmem[tid+1];  __syncwarp();
</code></pre>

<p>Listing 8 fixes the race condition by inserting extra __syncwarp() calls. The CUDA compiler may elide some of these synchronization instructions in the final generated code depending on the target architecture (e.g. on pre-Volta architectures).</p>

<pre class="line-numbers"><code class="language-cpp">unsigned tid = threadIdx.x;
int v = 0;

v += shmem[tid+16]; __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+8];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+4];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+2];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+1];  __syncwarp();
shmem[tid] = v;
</code></pre>

<p>On the latest Volta (and future) GPUs, you can also use __syncwarp() in thread-divergent branches to synchronize threads from both branches. But once they return from the primitive, the threads will become divergent again. See Listing 13 for such an example.</p>

<h2 id="toc_4">Opportunistic Warp-level Programming</h2>

<p>As we showed in the Synchronized Data Exchange section, the membership mask used in the synchronized data exchange primitives is often computed before a branch condition in the program flow. In many cases, the program needs to pass the mask along the program flow; for example, as a function argument when warp-level primitives are used inside a function. This may be difficult if you want to use warp-level programming inside a library function but you cannot change the function interface.</p>

<p>Some computations can use whatever threads happen to be executing together. We can use a technique called opportunistic warp-level programming, as the following example illustrates. (See this post on warp-aggregated atomics for more information on the algorithm, and this post for discussion of how Cooperative Groups makes the implementation much simpler.)</p>

<pre class="line-numbers"><code class="language-cpp">// increment the value at ptr by 1 and return the old value
__device__ int atomicAggInc(int *ptr) {
    int pred;
    int mask = __match_all_sync(__activemask(), ptr, &amp;pred);
    int leader = __ffs(mask) – 1;    // select a leader
    int res;
    if(lane_id() == leader)                  // leader does the update
        res = atomicAdd(ptr, __popc(mask));
    res = __shfl_sync(mask, res, leader);    // get leader’s old value
    return res + __popc(mask &amp; ((1 &lt;&lt; lane_id()) – 1)); //compute old value
}
</code></pre>

<p>atomicAggInc() atomically increments the value pointed to by ptr by 1 and returns the old value. It uses the atomicAdd() function, which may incur contention. To reduce contention, atomicAggInc replaces the per-thread atomicAdd() operation with a per-warp atomicAdd(). The __activemask() in line 4 finds the set of threads in the warp that are about to perform the atomic operation. __match_all_sync() returns the bit mask of the threads that have the same value ptr, partitioning the incoming threads into groups whose members have the same ptr value. Each group elects a leader thread (line 5), which performs the atomicAdd() (line 8) for the whole group. Every thread gets the old value from the leader (line 9) returned by the atomicAdd(). Line 10 computes and returns the old value the current thread would get from atomicInc() if it were to call the function instead of atomicAggInc.</p>

<h2 id="toc_5">Implicit Warp-Synchronous Programming is Unsafe</h2>

<p>CUDA toolkits prior to version 9.0 provided a (now legacy) version of warp-level primitives. Compared with the CUDA 9 primitives, the legacy primitives do not accept a mask argument. For example, int __any(int predicate) is the legacy version of int __any_sync(unsigned mask, int predicate).</p>

<p>The mask argument, as explained previously, specifies the set of threads in a warp that must participate in the primitives. The new primitives perform intra-warp thread-level synchronization if the threads specified by the mask are not already synchronized during execution.</p>

<p>The legacy warp-level primitives do not allow programmers to specify the required threads and do not perform synchronization. Therefore, the threads that must participate in the warp-level operation are not explicitly expressed by the CUDA program. The correctness of such a program depends on implicit warp-synchronous behavior, which may change from one hardware architecture to another, from one CUDA toolkit release to another (due to changes in compiler optimizations, for example), or even from one run-time execution to another. Such implicit warp-synchronous programming is unsafe and may not work correctly.</p>

<p>For example, in the following code, let’s assume all 32 threads in a warp execute line 2 together. The if statement at line 4 causes the threads to diverge, with the odd threads calling foo() at line 5 and the even threads calling bar() at line 8.</p>

<pre class="line-numbers"><code class="language-cpp">// Assuming all 32 threads in a warp execute line 1 together.
assert(__ballot(1) == FULL_MASK);
int result;
if (thread_id % 2) {
    result = foo();
}
else {
    result = bar();
}
unsigned ballot_result = __ballot(result);
</code></pre>

<p>The CUDA compiler and the hardware will try to re-converge the threads at line 10 for better performance. But this re-convergence is not guaranteed. Therefore, the ballot_result may not contain the ballot result from all 32 threads.</p>

<p>Calling the new __syncwarp() primitive at line 10 before __ballot(), as illustrated in Listing 11, does not fix the problem either. This is again implicit warp-synchronous programming. It assumes that threads in the same warp that are once synchronized will stay synchronized until the next thread-divergent branch. Although it is often true, it is not guaranteed in the CUDA programming model.</p>

<pre class="line-numbers"><code class="language-cpp">__syncwarp();
unsigned ballot_result = __ballot(result);
</code></pre>

<p>The correct fix is to use __ballot_sync() as in Listing 12.</p>

<pre class="line-numbers"><code class="language-cpp">unsigned ballot_result = __ballot_sync(FULL_MASK, result);
</code></pre>

<p>A common mistake is to assume that calling __syncwarp() before and/or after a legacy warp-level primitive is functionally equivalent to calling the sync version of the primitive. For example, is __syncwarp(); v = __shfl(0); __syncwarp(); the same as __shfl_sync(FULL_MASK, 0)? The answer is no, for two reasons. First, if the sequence is used in a thread-divergent branch, then __shfl(0) won’t be executed by all threads together. Listing 13 shows an example. The __syncwarp() at line 3 and line 7 would ensure foo() is called by all threads in the warp before line 4 or line 8 is executed. Once threads leave the __syncwarp(), the odd threads and the even threads become divergent again. Therefore, the __shfl(0) at line 4 will get an undefined value because lane 0 is inactive when line 4 is executed. __shfl_sync(FULL_MASK, 0) can be used in thread-divergent branches without this problem.</p>

<pre class="line-numbers"><code class="language-cpp">v = foo();
if (threadIdx.x % 2) {
    __syncwarp();
    v = __shfl(0);       // L3 will get undefined result because lane 0 
    __syncwarp();        // is not active when L3 is executed. L3 and L6
} else {                 // will execute divergently.
    __syncwarp();
    v = __shfl(0);
    __syncwarp();
}
</code></pre>

<p>Second, even when the sequence is called by all the threads together, the CUDA execution model does not guarantee threads will stay convergent after leaving __syncwarp(), as Listing 14 shows. Implicit lock-step execution is not guaranteed. Remember, thread convergence is guaranteed only within explicitly synchronous warp-level primitives.</p>

<pre class="line-numbers"><code class="language-cpp">assert(__activemask() == FULL_MASK); // assume this is true
__syncwarp();
assert(__activemask() == FULL_MASK); // this may fail
</code></pre>

<p>Because using them can lead to unsafe programs, the legacy warp-level primitives are deprecated starting in CUDA 9.0.</p>

<h2 id="toc_6">Update Legacy Warp-Level Programming</h2>

<p>If your program uses legacy warp-level primitives or any form of implicit warp-synchronous programming (such as communicating between threads of a warp without synchronization), you should  update the code to use the sync version of the primitives. You may also want to restructure your code to use Cooperative Groups, which provides a higher level of abstraction as well as new features such as multi-block synchronization.</p>

<p>The trickiest part of using the warp-level primitives is figuring out the membership mask to be used. We hope the above sections give you a good idea where to start and what to look out for.  Here is a list of suggestions:</p>

<ol>
<li>Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.</li>
<li>Don’t just use __activemask() as the mask value. __activemask() tells you what threads happen to be convergent when the function is called, which can be different from what you want to be in the collective operation.</li>
<li>Do analyze the program logic and understand the membership requirements. Compute the mask ahead based on your program logic.</li>
<li>If your program does opportunistic warp-synchronous programming, use “detective” functions such as __activemask() and __match_all_sync() to find the right mask.</li>
<li>Use __syncwarp() to separate operations with intra-warp dependences. Do not assume lock-step execution.</li>
</ol>

<p>One last trick. If your existing CUDA program gives a different result on Volta architecture GPUs, and you suspect the difference is caused by Volta’s new independent thread scheduling which can change warp synchronous behavior, you may want to recompile your program with nvcc options -arch=compute_60 -code=sm_70. Such compiled programs opt-in to Pascal’s thread scheduling. When used selectively, it can help pin down the culprit module more quickly, allowing you to update the code to avoid implicit warp-synchronous programming.</p>

<p><img src="media/15168532667210/15171479419797.png" alt=""/><br/>
<strong><div align = center><span id="image2">图片2：Volta independent thread scheduling enables interleaved execution of statements from divergent branches. This enables execution of fine-grain parallel algorithms where threads within a warp may synchronize and communicate.</span></div></strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='cuda.html'>CUDA</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180121_CUDA_Volatile_And_Memory_Fence.html">
                
                  <h1>Volatile 限定词（Qualifier）和内存围栏（memory fence)</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>这边blog是从nvidia blog上最新的文章<a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a> 引申而来。原文主要介绍了CUDA 9.0中引入的一些新的warp级别的原语(primitives)。本文中就里面提到一些概念进行引申讲解。</p>

<p>在讲到__syncwarp()时候提到了：</p>

<blockquote>
<p>The __syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed a __syncwarp() (with the same mask) before resuming execution. <strong>It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</strong></p>
</blockquote>

<p>那这个<strong>memory fence</strong>是一个什么概念呢？</p>

<h2 id="toc_0">内存围栏（memory fence)</h2>

<p>CUDA 编程模型假定了一种弱顺序(weakly-ordered)内存模型。也就是说一个CUDA thread 将数据写入共享内存(shared memory)，全局内存（global memory)，pinned CPU内存（page-locked host memory)，或者对等设备（peer device，可以与当前设备进行P2P通信的设备）的内存时，并不需要与数据被观察到的顺序一致。</p>

<p>用一个实际例子来说明：<br/>
假设下面的代码中，thread 1 执行writeXY()，而thread 2 执行readXY()，thread 2读取到的值是多少呢？</p>

<pre class="line-numbers"><code class="language-cpp">__device__ volatile int X = 1, Y = 2; 

__device__ void writeXY() { 
    X = 10; Y = 20; 
} 

__device__ void readXY() { 
    int A = X; int B = Y; 
}
</code></pre>

<p>如果是一个<strong>强顺序内存模型(strongly-ordered memory model)</strong>中，只有可能是</p>

<ul>
<li>A=1, B=2</li>
<li>A=10, B=2</li>
<li>A=10, B=20</li>
</ul>

<p>因为虽然两个threads的执行完成可能有先有后，但是A=X, B=Y的必须在语义学上是前后关系（也就是被观察顺序）。因此，上述三种情况分别是：</p>

<ul>
<li>A=1, B=2：thread 1 没有执行完X=10, Y=20的操作；thread 2 已经执行完了A=X, B=Y的操作。</li>
<li>A=10, B=2：thread 2 执行完X=10, 还没有执行Y=20；thread 2 已经执行完了A=X, B=Y的操作，所以Y的值还没有更新。</li>
<li>A=10, B=20：thread 1 执行完X=10, Y=20；thread 2读取了对应数据，X和Y都是最新的值。</li>
</ul>

<p>但是在CUDA使用的<strong>弱顺序内存模型</strong>中，可能会出现：</p>

<ul>
<li>A=1, B=20</li>
</ul>

<p>的情况。也就是说，虽然语义上A=10比B=20更早出现，但是CUDA的内存模型并不保证这一顺序。</p>

<p><strong>内存围栏函数可以被用来保证实际内存读取顺序和语义学顺序相同。</strong><br/>
在CUDA中提供了三个内存围栏函数：</p>

<pre class="line-numbers"><code class="language-cpp">void __threadfence_block();
void __threadfence();
void __threadfence_system();
</code></pre>

<p>这些函数都保证在函数调用前的内存操作不会晚于函数调用后的。</p>

<p>我们考虑一个比较常见的情景，这个技巧会在很多地方用到，大家也可以自己思考一下对这个问题有一些什么样的解决方案：<br/>
用一次核函数（kernel）调用计算N个数的和，也就是我们常说的reduction操作。我们实现的逻辑是：</p>

<ul>
<li>每个block首先计算这个数组的一个子集，并将计算结果写到全局内存中。提示，在改部分计算中可以使用warp原语、共享内存等方法来进一步加速，请参考相应的post。</li>
<li>当所有block都完成了这个计算，最后一个block从全局内存中读取这些部分和，并且将他们相加得到最终结果。</li>
<li>为了确认哪一个block最后完成操作。每一个block都会用原子加法(atomic add)将一个计数器（counter）加一。这样，最后一个block就是读取到counter值为gridDim.x-1的那个block。</li>
</ul>

<p>如果没有在存储部分和与计数器加一之间插入内存未来，计数器可能会在部分和存储完成之前加一，这样最终计算的结果可能会缺失一部分。</p>

<pre class="line-numbers"><code class="language-cpp">__device__ unsigned int count = 0; 
__shared__ bool isLastBlockDone; 

__global__ void sum(const float* array, unsigned int N, volatile float* result) 
{ 
    // Each block sums a subset of the input array. 
    float partialSum = calculatePartialSum(array, N); 
    
    if (threadIdx.x == 0) { 
        // Thread 0 of each block stores the partial sum 
        // to global memory. The compiler will use 
        // a store operation that bypasses the L1 cache 
        // since the &quot;result&quot; variable is declared as 
        // volatile. This ensures that the threads of 
        // the last block will read the correct partial 
        // sums computed by all other blocks. 
        result[blockIdx.x] = partialSum; 
        
        // Thread 0 makes sure that the incrementation 
        // of the &quot;count&quot; variable is only performed after 
        // the partial sum has been written to global memory. 
        __threadfence(); 
        
        // Thread 0 signals that it is done. 
        unsigned int value = atomicInc(&amp;count, gridDim.x); 
        
        // Thread 0 determines if its block is the last 
        // block to be done. 
        isLastBlockDone = (value == (gridDim.x - 1)); } 
        
        // Synchronize to make sure that each thread reads 
        // the correct value of isLastBlockDone. 
        __syncthreads(); 
        
        if (isLastBlockDone) { 
            // The last block sums the partial sums 
            // stored in result[0 .. gridDim.x-1] 
            float totalSum = calculateTotalSum(result); 
            
            if (threadIdx.x == 0) { 
                // Thread 0 of last block stores the total sum 
                // to global memory and resets the count 
                // varialble, so that the next kernel call 
                // works properly. 
                result[0] = totalSum; count = 0; 
            } 
        } 
    }
</code></pre>

<p>内存围栏只会影响一个线程中内存操作的顺序。他并不能像__syncthreads()那样保证这些内存操作对于同block中的其他线程可见。也就是说，由于有L1 cache的存在，其他block可能读取到的是没有更新的result值。在我们以往的经验中，这个是用__syncthreads()来保证的。在这个例子里，volatile限定词被用来保证内存操作的可见性。</p>

<h2 id="toc_1">volatile 限定词（Qualifier）</h2>

<p>从<a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#volatile-qualifier">CUDA编程手册</a>我们可以看到：</p>

<blockquote>
<p>The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions (Memory Fence Functions) and memory visibility semantics of synchronization functions (Synchronization Functions).</p>

<p>These optimizations can be disabled using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction.</p>
</blockquote>

<p>对于nvcc编译器来说，编译器是可以自由地优化全局内存(global memory)和共享内存(shared memory)的读写顺序。例如，这种优化可以是通过将共享内存的读取缓存到寄存器或者L1缓存中来实现的。但是，这种优化，必须要服从由内存围栏(memory fence)函数所定义出的内存顺序语义(memory ordering semantics)，还必须要服从同步函数(synchronization function)所限制的内存可见性语义（memory visibility semantics)。</p>

<p>如果我们对某一个内存（全局内存或共享内存）使用了volatile这个关键词，<strong>编译器将认为该内存的值允许在任意时刻被其他thread修改</strong>。因此，该变量的所有引用，都会被编译为真实的读写指令。</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='cuda.html'>CUDA</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180121_TF_to_TRT.html">
                
                  <h1>TensorFlow模型的保存，已经TensoRT中tensorFlow 模型的导入（草稿）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	
                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='dl.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180116_DL_Tuning_Reprint.html">
                
                  <h1>深度学习中的优化调参细节（转帖）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">深度学习中的技巧</h2>

<ul>
<li>初始化参数尽量小一些，这样 softmax 的回归输出更加接近均匀分布，使得刚开始网络并不确信数据属于哪一类；另一方面从数值优化上看我们希望我们的参数具有一致的方差（一致的数量级），这样我们的梯度下降法下降也会更快。同时为了使每一层的激励值保持一定的方差，我们在初始化参数（不包括偏置项）的方差可以与输入神经元的平方根成反比。</li>
<li>学习率（learning rate）的设置应该随着迭代次数的增加而减小，个人比较喜欢每迭代完一次epoch也就是整个数据过一遍，然后对学习率进行变化，这样能够保证每个样本得到了公平的对待。</li>
<li>滑动平均模型，在训练的过程中不断的对参数求滑动平均这样能够更有效的保持稳定性，使其对当前参数更新不敏感。例如加动量项的随机梯度下降法就是在学习率上应用滑动平均模型。</li>
<li>在验证集上微小的提升未必可信，一个常用的准则是增加了30个以上的正确样本，能够比较确信算法有了一定的提升。 </li>
<li>不要太相信模型开始的学习速度，这与最终的结果基本没有什么关系。一个低的学习速率往往能得到较好的模型。</li>
<li>在深度学习中，常用的防止过拟合的方法除了正则化，dropout和pooling之外，还有提前停止训练的方法——就是看到我们在验证集的上的正确率开始下降就停止训练。</li>
<li>当激活函数是RELU时，我们在初始化偏置项时，为了避免过多的死亡节点（激活值为0）一般可以初始化为一个较小的正值。</li>
<li>基于随机梯度下降的改进优化算法有很多种，在不熟悉调参的情况，建议使用Adam方法</li>
<li>训练过程不仅要观察训练集和测试集的loss是否下降、正确率是否提高，对于参数以及激活值的分布情况也要及时观察，要有一定的波动。</li>
<li>如果我们设计的网络不work，在训练集的正确率也很低的话，我们可以减小样本数量同时去掉正则化项，然后进行调参，如果正确率还是不高的话，就说明我们设计的网络结果可能有问题。</li>
<li>fine-tuning的时候，可以把新加层的学习率调高，重用层的学习率可以设置的相对较低。</li>
<li>在隐藏层的激活函数，tanh往往比sigmoid表现更好。</li>
<li>针对梯度爆炸的情况我们可以使用梯度截断来解决，尤其在RNN中由于存在相同的循环结构，导致相同参数矩阵的连乘，更加容易产生梯度爆炸。当然，使用LSTM和GRU等更加优化的模型往往是更好地选择。</li>
<li>正则化输入，也就是让特征都保持在0均值和1方差。（注意做特征变换时请保持训练集合测试集进行了相同的变化）</li>
<li>梯度检验：当我们的算法在训练出现问题而进行debug时，可以考虑使用近似的数值梯度和计算的梯度作比较检验梯度是否计算正确。</li>
<li>搜索超参数时针对经典的网格搜索方法，这里有两点可以改善的地方：1）不用网格，用随机值，因为这样我们一次实验参数覆盖范围更广，尤其在参数对结果影响级别相差很大的情况下。2）不同数量级的搜索密度是不一样的，不能均分。</li>
</ul>

<h2 id="toc_1">CNN中的独特技巧</h2>

<ul>
<li>CNN中将一个大尺寸的卷积核可以分解为多层的小尺寸卷积核或者分成多层的一维卷积。这样能够减少参数增加非线性。</li>
<li>CNN中的网络设计应该是逐渐减小图像尺寸，同时增加通道数，让空间信息转化为高阶抽象的特征信息。</li>
<li>CNN中可以利用Inception方法来提取不同抽象程度的高阶特征，用ResNet的思想来加深网络的层数。</li>
<li>CNN处理图像时，常常会对原图进行旋转、裁剪、亮度、色度、饱和度等变化以增大数据集增加鲁棒性。</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/16</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='dl.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180201_TRT_Plugin_API.html">
                
                  <h1>TensorRT Plugin APIs 介绍（草稿）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>TensorRT 中一个非常重要的特性是对于Plugin的支持。目前来说，标准Caffe中的CNN相关神经网络层已经完全支持。但是有很多改动版本的Caffe里面新添加的网络层是不支持的。例如SSD网络里面的priorbox, flatten, permute, detecion_out等。这些网络层在tensorRT里面可以使用Plugin的方式来支持。<br/>
因为整体上说，tensorRT的优化主要针对CBR (convolution-bias-relu)的合并，卷积的优化计算，低精度（int8，fp16）也主要是针对卷积层进行的。因此少量的plugin不会对性能产生较大影响。<br/>
但是对于YOLOv2这个网络，使用了目前tensorRT里面还不直接支持的PReLU, 如果把这个层使用plugin来实现，会出现CBR过程被打断，一些优化会受到影响，整体性能提高幅度可能不大。</p>

<p>本文先简要介绍一下定义一个plugin需要重载的IPlugin里面的函数。</p>

<p>总体上来说，TensorRT运行分为若干个步骤：</p>

<ol>
<li>创建网络(create network)，这一步可以是通过caffe 的parser，也就是以caffe的prototxt和caffemodel作为输入。nvCafferParser 回去分析这个网络结构，并且解析出其中各个网络层，如果是tensorRT已经支持的网络层，则会调用相应的tensorRT的API来添加到tensorRT的network中。如果是没有的网络层，但是已经在PluginFactory工厂里面定义了（通过isPlugin(layerName)来判断），则会标记为plugin添加到network中。</li>
<li>创建TensorRT引擎(create TensorRT engine)，这一步是tensorRT回去根据配置生成优化engine。主要的优化项目包括：合并网络层（例如CBR的合并，多个相同大小的CBR进行进一步合并），消除多余的网络层（例如concat层），根据提供的最大Batch数(maxium batch size)，workspace来选择相应的算法和CUDA实现(kernel)，预分配中间变量、输入输出buffer等。</li>
<li>运行时（runtime), 在这个阶段，</li>
</ol>

<h2 id="toc_0">When creating the network (from parser or API)</h2>

<ul>
<li>getNbOutputs()</li>
<li>getOutputDimensions()</li>
</ul>

<h2 id="toc_1">By the builder</h2>

<ul>
<li>configure()</li>
<li>getWorkSpaceSize()</li>
</ul>

<h2 id="toc_2">At runtime</h2>

<ul>
<li>initialize() when the engine context is constructed</li>
<li>enqueue() at inference time</li>
<li>terminate() when the engine context is destroyed</li>
</ul>

<h2 id="toc_3">For serialization</h2>

<ul>
<li>getSerializationSize()</li>
<li>serialize()</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/2</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='trt.html'>TensorRT</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="all.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_2.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="media/15161603048345/Blog_LOGO.png" /></div>
            
                <h1>lxp121's BLOG on CUDA and DL</h1>
                <div class="site-des"></div>
                <div class="social">



<a target="_blank" class="linkedin" href="https://www.linkedin.com/in/xipengli" title="LinkedIn">LinkedIn</a>





<a target="_blank" class="github" target="_blank" href="https://github.com/lxp121" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lxp121@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="dl.html"><strong>Deep Learning</strong></a>
        
            <a href="cuda.html"><strong>CUDA</strong></a>
        
            <a href="trt.html"><strong>TensorRT</strong></a>
        
            <a href="Git.html"><strong>Git</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15262740803283.html">关于TRT里面双向RNN参数的设置 [待更新]</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15231593087199.html">TensorFlow-TensorRT 的使用及其限制（草稿）</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15203065717420.html">GPU代码的向后兼容问题（草稿）</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180212_git_gitignore.html">Git 修改.gitignore不起作用</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180212_git_move_commits_to_new_branch.html">Git 将最近改动移到新的分支(branch)上</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
