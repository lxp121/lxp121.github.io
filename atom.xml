<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[lxp121's BLOG on CUDA and DL]]></title>
  <link href="china.xipengli.com/atom.xml" rel="self"/>
  <link href="china.xipengli.com/"/>
  <updated>2018-02-12T16:17:11+08:00</updated>
  <id>china.xipengli.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[Git 修改.gitignore不起作用]]></title>
    <link href="china.xipengli.com/20180212_git_gitignore.html"/>
    <updated>2018-02-12T16:05:44+08:00</updated>
    <id>china.xipengli.com/20180212_git_gitignore.html</id>
    <content type="html"><![CDATA[
<p>.gitignore 只能忽略那些没有被track的文件，如果这些文件以及在版本管理里面了（通过git add)，则.gitignore是无效的。办法是删除现在的track状态（改变为未track），从新提交：</p>

<pre><code class="language-bash">git rm -r --cached .
git add .
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Git 将最近改动移到新的分支(branch)上]]></title>
    <link href="china.xipengli.com/20180212_git_move_commits_to_new_branch.html"/>
    <updated>2018-02-12T13:06:17+08:00</updated>
    <id>china.xipengli.com/20180212_git_move_commits_to_new_branch.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">背景及 <em>错误</em> workflow</h2>

<p>我们有时候会发现针对某个项目做了大量改动，在push的时候发现自己并没有push权限。当然，这也不是正常的操作流程，正常的流程应该是：</p>

<pre><code class="language-flow">st=&gt;start: fork 一个新的分支
e=&gt;end: 推到服务器端
op=&gt;operation: 在新分支上做相应修改
op2=&gt;operation: 提交到新的服务器
op3=&gt;operation: 提交merge request
op4=&gt;operation: master 同意merge request，合并到现有branch中

st-&gt;op-&gt;op2-&gt;op3-&gt;op4-&gt;e
</code></pre>

<p>但是这样的操作有时候会被忘记，在已经提交了多个request之后，如何修改呢？<br/>
网上错误的教程是这样的：</p>

<pre><code class="language-bash"># WRONG workflow, not test!!
git branch -t newbranch
git reset --hard HEAD~3
git checkout newbranch
</code></pre>

<p>这样的后果是你下一次运行git rebase (或者git pull --rebase)的时候，这三次提交会消失。</p>

<h2 id="toc_1">正确的workflow</h2>

<pre><code class="language-bash">git reset --keep HEAD~3
git checkout -t -b newbranch
git cherry-pick ..HEAD@{2}
</code></pre>

<p>这三行代码进行了如下工作：</p>

<ul>
<li>首先它抛弃了最近三次提交。--keep 和 --hard很相似，但是更加安全一点，如果出现错误，不会丢弃没有提交的改动。</li>
<li>创建一个新的branch。</li>
<li>最后，使用cherry-picks 这个三个提交到新的分支 newbranch 上。因为这三个提交不再被任何一个分支引用，我们通过<em>relog</em>命令: <em>HEAD@{2}</em> 表示HEAD 两次操作(checkout 和reset)之前的提交。</li>
</ul>

<p>通常<em>reflog</em>是默认开启的，但是如果手动关闭了，则不能通过..HEAD@{2}找回三次之前的提交。</p>

<p>一个替代的方法是:</p>

<pre><code class="language-bash"># newbranch will omit the 3 most recent commits.
git checkout -b newbranch HEAD~3
git branch --set-upstream-to=oldbranch
# Cherry-picks the extra commits from oldbranch.
git cherry-pick ..oldbranch
# Discards the 3 most recent commits from oldbranch.
git branch --force oldbranch oldbranch~3
</code></pre>

<h2 id="toc_2">技术解释</h2>

<p>在上面的例子中，为什么 <em>git rebase</em> 会抛弃三次提交呢？这是因为<em>git rebase</em> 默认不会使用--fork-point 选项。</p>

<p>假设你有master分支包含M1，M2和M3三个提交。</p>

<pre><code>M1--M2--M3  &lt;-- origin/master
         \
          T1--T2--T3  &lt;-- topic
</code></pre>

<p>现在，如果有人修改了history，并且使用force-pushing 删除了M2</p>

<pre><code>M1--M3&#39;  &lt;-- origin/master
 \
  M2--M3--T1--T2--T3  &lt;-- topic
</code></pre>

<p>此时因为M3&#39;并不是分支topic里面的元素，因此git rebase 会认为T1-&gt;T3 已经不再需要（上游提交已经被删除）。</p>

<pre><code>M1--M3&#39;  &lt;-- origin/master
     \
      T1&#39;--T2&#39;--T3&#39;  &lt;-- topic (rebased)
</code></pre>

<p>这也是合理的操作。</p>

<p>因此在之前的例子中，</p>

<pre><code class="language-bash">git branch -t newbranch
git reset --hard HEAD~3
git checkout newbranch
</code></pre>

<p>Git将会把newbranch看做是从上游fork出来的一个分支，并且包含三个commits。然后<em>reset --hard</em> 会重写upstream的历史，并且删除三个提交。下一次你使用<em>git rebase</em>, 这三个提交会像其他从upstream里面删除的commit一样被抛弃掉。</p>

<p>但是对于我们这个情况，我们认为这三个commit应该是topic分支的一部分。 因此我们需要从upstream 更早的位置fork（不包含3个提交）。</p>

<h3 id="toc_3">注释</h3>

<p>因为大多数人不会去修改master branch, 因此即使错误的方法也没有很大的问题。</p>

<h3 id="toc_4">复现</h3>

<p>使用:</p>

<pre><code>touch M1 &amp;&amp; git add * &amp;&amp; git commit -m &quot;M1&quot;
touch M2 &amp;&amp; git add * &amp;&amp; git commit -m &quot;M2&quot;
touch M3 &amp;&amp; git add * &amp;&amp; git commit -m &quot;M3&quot;

git checkout -b topic
touch T1 &amp;&amp; git add * &amp;&amp; git commit -m &quot;T1&quot;
touch T2 &amp;&amp; git add * &amp;&amp; git commit -m &quot;T2&quot;
touch T3 &amp;&amp; git add * &amp;&amp; git commit -m &quot;T3&quot;

git checkout master
git reset HEAD~2
touch M3_primer &amp;&amp; rm M2 M3 &amp;&amp; git commit -m &quot;M3&#39;&quot;
git push --force

git checkout topic
git rebase master
</code></pre>

<ul>
<li>创建M1-&gt;M2-&gt;M3，此时master 包含文件: M1, M2, M3</li>
<li>创建topic 分支，此时topic包含：M1, M2, M3, T1, T2, T3</li>
<li>在master分支上reset到M1，并删除M2和M3两个文件，提交。</li>
<li>使用git push --froce提交到远端。</li>
<li>使用git checkout topic, git rebase master。会得到T3为M1, M3&#39;, T1, T2, T3。对于我们的情况是不对的（我们需要T1, T2, T3, M1, M2, M3)。但是逻辑上是正确的，因为你的base变了，会认为是新的T1&#39;，T2&#39;，T3&#39;。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用 CUDA 的 warp-level 原语]]></title>
    <link href="china.xipengli.com/20180125_CUDA_Warp_Level_Primitives.html"/>
    <updated>2018-01-25T12:07:46+08:00</updated>
    <id>china.xipengli.com/20180125_CUDA_Warp_Level_Primitives.html</id>
    <content type="html"><![CDATA[
<p>本文是NVIDA Blog文章<a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a>的全文翻译和深度解析。</p>

<p>从CUDA 9.0 开始，CUDA引入了更加灵活的group的选择，这一方面使得CUDA编程更加简单，一方面也使得一些原有的功能发生了一些改变。本文重点对warp级别的原语（primitives）进行一些介绍。</p>

<p>在GPU中，线程（thread）被组织为warp，然后warp会按照SIMT（单指令多线程，Single Instruction Multiple Thread）的形式来执行。很多CUDA程序都可以通过充分利用warp执行来达到很高的性能。</p>

<h2 id="toc_0">Warp级别原语（Warp-level Primitives）</h2>

<p>NVIDIA GPUs和CUDA编程模型使用一种被称为SIMT的执行模式。SIMT和SIMD（Single Instruction, Multiple Data)的区别主要是：</p>

<ul>
<li>在SIMD架构中，每个指令在不同的数据上并行的进行相同的操作。SIMD通常采用向量寄存器和向量执行单元来实现。而vector执行是通过一个标量的向量来实现的。简言之，一个线程调度，多个向量寄存器和执行单元上并行完成相同指令。</li>
<li>而SIMT架构中，并不只是使用单一的线程，并且数据也不要求采用向量存储方式。多个线程会发起普通的指令（而非向量指令）在任意数据上执行。我们可以想象一下GPU的执行时候，可以根据每个thread的线程来计算需要操作的数据位置。实际上，可以再SIMT的架构中进一步实现SIMD的子架构，以Pascal架构中支持的INT8计算模式为例。该模式可以非常高效地提高深度神经网络的计算。在INT8的计算中，要求被计算的4个数据相互邻近（每个8bit），4个指令在一个GPU线程上执行。而这个线程是SIMT中的一个线程。</li>
</ul>

<p>在NVIDIA的GPU上，32个并行线程（相邻线程）被组成一个warp，每一个线程可以访问它自己的寄存器，从不同地址（可以不相邻）读写数据。并且，这些线程是可以支持分歧的控制流路径（divergent control flow paths)。本文中，我们重点放在使用Warp级别原语的使用。</p>

<p><em><a href="#code1">代码1</a></em> 是一个warp-level 原语的例子。我们使用了一个<strong>__shfl_down_sync()</strong> 来实现一个tree-reduction方式来计算一个warp中线程val值得和。该代码执行完之后，warp中的第一个线程中的val值就等于最终的和。</p>

<p><strong><div align = lift><span id="code1">代码1: warp-level reduction</span></div></strong> </p>

<pre><code class="language-cpp">#define FULL_MASK 0xffffffff
for (int offset = 16; offset &gt; 0; offset /= 2)
    val += __shfl_down_sync(FULL_MASK, val, offset);
</code></pre>

<p>这个计算可以用下面的图来说明：<br/>
<img src="media/15168532667210/15171231191469.png" alt="使用__shlf_down_sync()来实现warp级别的并行reduction"/><br/>
<strong><div align = center><span id="image1">图片1：使用__shlf_down_sync()来实现warp级别的并行reduction</span></div></strong></p>

<p>一个warp中包含32通道（lane），每一个线程占用一个通道。对于处于 X 通道的线程，<strong>__shlf_down_sync(FULL_MASK, val, offset)</strong> 来获得在同一个warp中的第 <strong>X+offset</strong> 航道的val值。这一数据交换在regitster中进行的，因此其效率高于共享内存。使用共享内存需要一次读，一次写，并且需要一个额外的寄存器来保存这些地址。</p>

<p>CUDA 9中包含了三类warp级别原语：</p>

<ol>
<li>同步地数据交换（Synchronized data exchange）：在同一warp中不同thread之间交换数据：

<ul>
<li>__all_sync, __any_sync, __uni_sync, __ballot_sync</li>
<li>__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</li>
<li>__match_any_sync, __match_all_sync</li>
</ul></li>
<li>Active mask query: 返回一个32-bit的mask来表示warp中哪些thread是活动的。

<ul>
<li>__activemask</li>
</ul></li>
<li>线程同步（Thread synchronization)：同步warp中的线程，并提供一个内存围栏（关于内存围栏的概念，可以在<a href="20180121_CUDA_Volatile_And_Memory_Fence.html">Volatile 限定词（Qualifier）和内存围栏（memory fence) </a>中找到详细解释）。</li>
</ol>

<h2 id="toc_1">同步地数据交换（Synchronized data exchange）</h2>

<p>每一个”同步地数据交换“原语都会在warp中的一组线程（不一定是所有线程）上执行一个聚合操作（collective operation)。</p>

<p>Each of the “synchronized data exchange” primitives perform a collective operation among a set of threads in a warp. For example, Listing 2 shows three of these. Each thread that calls __shfl_sync() or __shfl_down_sync() receives data from a thread in the same warp, and each thread that calls __ballot_sync() receives a bit mask representing all the threads in the warp that pass a true value for the predicate argument.</p>

<p><strong><div align = lift><span id="code2">代码2: Synchronized data exchange warp-level primitives</span></div></strong> </p>

<pre><code class="language-cpp">int __shfl_sync(unsigned mask, int val, int src_line, int width=warpSize);
int __shfl_down_sync(unsigned mask, int var, unsigned detla, 
                     int width=warpSize);
int __ballot_sync(unsigned mask, int predicate);
</code></pre>

<p>The set of threads that participates in invoking each primitive is specified using a 32-bit mask, which is the first argument of these primitives. All the participating threads must be synchronized for the collective operation to work correctly. Therefore, these primitives first synchronize the threads if they are not already synchronized.</p>

<p>A frequently asked question is “what should I use for the mask argument?”. You can consider the mask to mean the set of threads in the warp that should participate in the collective operation. This set of threads is determined by the program logic, and can usually be computed by some branch condition earlier in the program flow. Take the reduction code in Listing 1 as an example. Assume we want to compute the sum of all the elements of an array input[], whose size NUM_ELEMENTS is less than the number of threads in the thread block. We can use the method in Listing 3.</p>

<p><strong><div align = lift><span id="code3">代码3: </span></div></strong> </p>

<pre><code class="language-cpp">unsigned mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);
if (threadIdx.x &lt; NUM_ELEMENTS) { 
    val = input[threadIdx.x]; 
    for (int offset = 16; offset &gt; 0; offset /= 2)
        val += __shfl_down_sync(mask, val, offset);
    …
}
</code></pre>

<p>The code uses the condition thread.idx.x &lt; NUM_ELEMENTS to determine whether or not a thread will participate in the reduction.  __ballot_sync() is used to compute the membership mask for the __shfl_down_sync() operation. __ballot_sync() itself uses FULL_MASK (0xffffffff for 32 threads) because we assume all threads will execute it.</p>

<p>On Volta and later GPU architectures, the data exchange primitives can be used in thread-divergent branches: branches where some threads in the warp take a different path than the others. Listing 4 shows an example where all the threads in a warp get the value of val from the thread at lane 0. The even- and odd-numbered threads take different branches of an if statement.</p>

<p><strong><div align = lift><span id="code4">代码4: </span></div></strong> </p>

<pre><code class="language-cpp">if (threadIdx.x % 2) {
    val += __shfl_sync(FULL_MASK, val, 0);
…
}
else {
val += __shfl_sync(FULL_MASK, val, 0);
…
}
</code></pre>

<p>On the latest Volta (and future) GPUs, you can run library functions that use warp synchronous primitives without worrying whether the function is called in a thread-divergent branch.</p>

<h2 id="toc_2">Active Mask Query</h2>

<p>__activemask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same __activemask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.</p>

<p>However, it’s important to use __active_mask() correctly. Listing 5 illustrates an incorrect use. The code tries to perform the same sum reduction  shown in Listing 4, but instead of using __ballot_sync() to compute the mask before the branch, it uses __active_mask() inside the branch. This is incorrect, as it would result in partial sums instead of a total sum. The CUDA execution model does not guarantee that all threads taking the branch together will execute the __active_mask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>

<p><strong><div align = lift><span id="code5">代码5: </span></div></strong> </p>

<pre><code class="language-cpp">//
// Incorrect use of __active_mask()
//
if (threadIdx.x &lt; NUM_ELEMENTS) { 
    unsigned mask = __active_mask(); 
    val = input[threadIdx.x]; 
    for (int offset = 16; offset &gt; 0; offset /= 2)
        val += __shfl_down_sync(mask, val, offset);
    …
}
</code></pre>

<h2 id="toc_3">Warp 同步（Synchronization）</h2>

<p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the __syncwarp() primitive to synchronize threads in a warp. It is similar to the __syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.</p>

<pre><code class="language-cpp">void __syncwarp(unsigned mask=FULL_MASK);
</code></pre>

<p>The __syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed a __syncwarp() (with the same mask) before resuming execution. It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</p>

<p>Listing 6 shows an example of shuffling the ownership of matrix elements among threads in a warp.</p>

<pre><code class="language-cpp">float val = get_value(…);
__shared__ float smem[4][8];
 
//   0  1  2  3  4  5  6  7 
//   8  9 10 11 12 13 14 15 
//  16 17 18 19 20 21 22 23
//  24 25 26 27 28 29 30 31
int x1 = threadIdx.x % 8;
int y1 = threadIdx.x / 8;
 
//   0  4  8 12 16 20 24 28
//   1  5 10 13 17 21 25 29
//   2  6 11 14 18 22 26 30
//   3  7 12 15 19 23 27 31
int x2= threadIdx.x / 4;
int y2 = threadIdx.x % 4;
 
smem[y1][x1] = val;
__syncwarp();
val = smem[y2][x2];
 
use(val);
</code></pre>

<p>Assume a 1-D thread block is used (i.e. threadIdx.y is always 0). At the beginning of the code, each thread in a warp owns one element of a 4×8 matrix with row-major indexing. In other words, lane 0 owns [0][0] and lane 1 owns [0][1]. Each thread stores its value into the corresponding position of a 4×8 array in shared memory. Then __syncwarp() is used to ensure all threads have done the store, before each thread reads from a transposed position in the array. In the end, each thread in the warp owns one element of the matrix with column-major indexing: lane 0 owns [0][0] and lane 1 owns [1][0].</p>

<p>Make sure that __syncwarp() separates shared memory reads and writes to avoid race conditions. Listing 7 illustrates an incorrect use in a tree sum reduction in shared memory. There is a shared memory read followed by a shared memory write between every two __syncwarp() calls. The CUDA programming model does not guarantee that all the reads will be performed before all the writes, so there is a race condition.</p>

<pre><code class="language-cpp">unsigned tid = threadIdx.x;

// Incorrect use of __syncwarp()
shmem[tid] += shmem[tid+16]; __syncwarp();
shmem[tid] += shmem[tid+8];  __syncwarp();
shmem[tid] += shmem[tid+4];  __syncwarp();
shmem[tid] += shmem[tid+2];  __syncwarp();
shmem[tid] += shmem[tid+1];  __syncwarp();
</code></pre>

<p>Listing 8 fixes the race condition by inserting extra __syncwarp() calls. The CUDA compiler may elide some of these synchronization instructions in the final generated code depending on the target architecture (e.g. on pre-Volta architectures).</p>

<pre><code class="language-cpp">unsigned tid = threadIdx.x;
int v = 0;

v += shmem[tid+16]; __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+8];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+4];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+2];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+1];  __syncwarp();
shmem[tid] = v;
</code></pre>

<p>On the latest Volta (and future) GPUs, you can also use __syncwarp() in thread-divergent branches to synchronize threads from both branches. But once they return from the primitive, the threads will become divergent again. See Listing 13 for such an example.</p>

<h2 id="toc_4">Opportunistic Warp-level Programming</h2>

<p>As we showed in the Synchronized Data Exchange section, the membership mask used in the synchronized data exchange primitives is often computed before a branch condition in the program flow. In many cases, the program needs to pass the mask along the program flow; for example, as a function argument when warp-level primitives are used inside a function. This may be difficult if you want to use warp-level programming inside a library function but you cannot change the function interface.</p>

<p>Some computations can use whatever threads happen to be executing together. We can use a technique called opportunistic warp-level programming, as the following example illustrates. (See this post on warp-aggregated atomics for more information on the algorithm, and this post for discussion of how Cooperative Groups makes the implementation much simpler.)</p>

<pre><code class="language-cpp">// increment the value at ptr by 1 and return the old value
__device__ int atomicAggInc(int *ptr) {
    int pred;
    int mask = __match_all_sync(__activemask(), ptr, &amp;pred);
    int leader = __ffs(mask) – 1;    // select a leader
    int res;
    if(lane_id() == leader)                  // leader does the update
        res = atomicAdd(ptr, __popc(mask));
    res = __shfl_sync(mask, res, leader);    // get leader’s old value
    return res + __popc(mask &amp; ((1 &lt;&lt; lane_id()) – 1)); //compute old value
}
</code></pre>

<p>atomicAggInc() atomically increments the value pointed to by ptr by 1 and returns the old value. It uses the atomicAdd() function, which may incur contention. To reduce contention, atomicAggInc replaces the per-thread atomicAdd() operation with a per-warp atomicAdd(). The __activemask() in line 4 finds the set of threads in the warp that are about to perform the atomic operation. __match_all_sync() returns the bit mask of the threads that have the same value ptr, partitioning the incoming threads into groups whose members have the same ptr value. Each group elects a leader thread (line 5), which performs the atomicAdd() (line 8) for the whole group. Every thread gets the old value from the leader (line 9) returned by the atomicAdd(). Line 10 computes and returns the old value the current thread would get from atomicInc() if it were to call the function instead of atomicAggInc.</p>

<h2 id="toc_5">Implicit Warp-Synchronous Programming is Unsafe</h2>

<p>CUDA toolkits prior to version 9.0 provided a (now legacy) version of warp-level primitives. Compared with the CUDA 9 primitives, the legacy primitives do not accept a mask argument. For example, int __any(int predicate) is the legacy version of int __any_sync(unsigned mask, int predicate).</p>

<p>The mask argument, as explained previously, specifies the set of threads in a warp that must participate in the primitives. The new primitives perform intra-warp thread-level synchronization if the threads specified by the mask are not already synchronized during execution.</p>

<p>The legacy warp-level primitives do not allow programmers to specify the required threads and do not perform synchronization. Therefore, the threads that must participate in the warp-level operation are not explicitly expressed by the CUDA program. The correctness of such a program depends on implicit warp-synchronous behavior, which may change from one hardware architecture to another, from one CUDA toolkit release to another (due to changes in compiler optimizations, for example), or even from one run-time execution to another. Such implicit warp-synchronous programming is unsafe and may not work correctly.</p>

<p>For example, in the following code, let’s assume all 32 threads in a warp execute line 2 together. The if statement at line 4 causes the threads to diverge, with the odd threads calling foo() at line 5 and the even threads calling bar() at line 8.</p>

<pre><code class="language-cpp">// Assuming all 32 threads in a warp execute line 1 together.
assert(__ballot(1) == FULL_MASK);
int result;
if (thread_id % 2) {
    result = foo();
}
else {
    result = bar();
}
unsigned ballot_result = __ballot(result);
</code></pre>

<p>The CUDA compiler and the hardware will try to re-converge the threads at line 10 for better performance. But this re-convergence is not guaranteed. Therefore, the ballot_result may not contain the ballot result from all 32 threads.</p>

<p>Calling the new __syncwarp() primitive at line 10 before __ballot(), as illustrated in Listing 11, does not fix the problem either. This is again implicit warp-synchronous programming. It assumes that threads in the same warp that are once synchronized will stay synchronized until the next thread-divergent branch. Although it is often true, it is not guaranteed in the CUDA programming model.</p>

<pre><code class="language-cpp">__syncwarp();
unsigned ballot_result = __ballot(result);
</code></pre>

<p>The correct fix is to use __ballot_sync() as in Listing 12.</p>

<pre><code class="language-cpp">unsigned ballot_result = __ballot_sync(FULL_MASK, result);
</code></pre>

<p>A common mistake is to assume that calling __syncwarp() before and/or after a legacy warp-level primitive is functionally equivalent to calling the sync version of the primitive. For example, is __syncwarp(); v = __shfl(0); __syncwarp(); the same as __shfl_sync(FULL_MASK, 0)? The answer is no, for two reasons. First, if the sequence is used in a thread-divergent branch, then __shfl(0) won’t be executed by all threads together. Listing 13 shows an example. The __syncwarp() at line 3 and line 7 would ensure foo() is called by all threads in the warp before line 4 or line 8 is executed. Once threads leave the __syncwarp(), the odd threads and the even threads become divergent again. Therefore, the __shfl(0) at line 4 will get an undefined value because lane 0 is inactive when line 4 is executed. __shfl_sync(FULL_MASK, 0) can be used in thread-divergent branches without this problem.</p>

<pre><code class="language-cpp">v = foo();
if (threadIdx.x % 2) {
    __syncwarp();
    v = __shfl(0);       // L3 will get undefined result because lane 0 
    __syncwarp();        // is not active when L3 is executed. L3 and L6
} else {                 // will execute divergently.
    __syncwarp();
    v = __shfl(0);
    __syncwarp();
}
</code></pre>

<p>Second, even when the sequence is called by all the threads together, the CUDA execution model does not guarantee threads will stay convergent after leaving __syncwarp(), as Listing 14 shows. Implicit lock-step execution is not guaranteed. Remember, thread convergence is guaranteed only within explicitly synchronous warp-level primitives.</p>

<pre><code class="language-cpp">assert(__activemask() == FULL_MASK); // assume this is true
__syncwarp();
assert(__activemask() == FULL_MASK); // this may fail
</code></pre>

<p>Because using them can lead to unsafe programs, the legacy warp-level primitives are deprecated starting in CUDA 9.0.</p>

<h2 id="toc_6">Update Legacy Warp-Level Programming</h2>

<p>If your program uses legacy warp-level primitives or any form of implicit warp-synchronous programming (such as communicating between threads of a warp without synchronization), you should  update the code to use the sync version of the primitives. You may also want to restructure your code to use Cooperative Groups, which provides a higher level of abstraction as well as new features such as multi-block synchronization.</p>

<p>The trickiest part of using the warp-level primitives is figuring out the membership mask to be used. We hope the above sections give you a good idea where to start and what to look out for.  Here is a list of suggestions:</p>

<ol>
<li>Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.</li>
<li>Don’t just use __activemask() as the mask value. __activemask() tells you what threads happen to be convergent when the function is called, which can be different from what you want to be in the collective operation.</li>
<li>Do analyze the program logic and understand the membership requirements. Compute the mask ahead based on your program logic.</li>
<li>If your program does opportunistic warp-synchronous programming, use “detective” functions such as __activemask() and __match_all_sync() to find the right mask.</li>
<li>Use __syncwarp() to separate operations with intra-warp dependences. Do not assume lock-step execution.</li>
</ol>

<p>One last trick. If your existing CUDA program gives a different result on Volta architecture GPUs, and you suspect the difference is caused by Volta’s new independent thread scheduling which can change warp synchronous behavior, you may want to recompile your program with nvcc options -arch=compute_60 -code=sm_70. Such compiled programs opt-in to Pascal’s thread scheduling. When used selectively, it can help pin down the culprit module more quickly, allowing you to update the code to avoid implicit warp-synchronous programming.</p>

<p><img src="media/15168532667210/15171479419797.png" alt=""/><br/>
<strong><div align = center><span id="image2">图片2：Volta independent thread scheduling enables interleaved execution of statements from divergent branches. This enables execution of fine-grain parallel algorithms where threads within a warp may synchronize and communicate.</span></div></strong></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Volatile 限定词（Qualifier）和内存围栏（memory fence)]]></title>
    <link href="china.xipengli.com/20180121_CUDA_Volatile_And_Memory_Fence.html"/>
    <updated>2018-01-21T16:28:11+08:00</updated>
    <id>china.xipengli.com/20180121_CUDA_Volatile_And_Memory_Fence.html</id>
    <content type="html"><![CDATA[
<p>这边blog是从nvidia blog上最新的文章<a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a> 引申而来。原文主要介绍了CUDA 9.0中引入的一些新的warp级别的原语(primitives)。本文中就里面提到一些概念进行引申讲解。</p>

<p>在讲到__syncwarp()时候提到了：</p>

<blockquote>
<p>The __syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed a __syncwarp() (with the same mask) before resuming execution. <strong>It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</strong></p>
</blockquote>

<p>那这个<strong>memory fence</strong>是一个什么概念呢？</p>

<h2 id="toc_0">内存围栏（memory fence)</h2>

<p>CUDA 编程模型假定了一种弱顺序(weakly-ordered)内存模型。也就是说一个CUDA thread 将数据写入共享内存(shared memory)，全局内存（global memory)，pinned CPU内存（page-locked host memory)，或者对等设备（peer device，可以与当前设备进行P2P通信的设备）的内存时，并不需要与数据被观察到的顺序一致。</p>

<p>用一个实际例子来说明：<br/>
假设下面的代码中，thread 1 执行writeXY()，而thread 2 执行readXY()，thread 2读取到的值是多少呢？</p>

<pre><code class="language-cpp">__device__ volatile int X = 1, Y = 2; 

__device__ void writeXY() { 
    X = 10; Y = 20; 
} 

__device__ void readXY() { 
    int A = X; int B = Y; 
}
</code></pre>

<p>如果是一个<strong>强顺序内存模型(strongly-ordered memory model)</strong>中，只有可能是</p>

<ul>
<li>A=1, B=2</li>
<li>A=10, B=2</li>
<li>A=10, B=20</li>
</ul>

<p>因为虽然两个threads的执行完成可能有先有后，但是A=X, B=Y的必须在语义学上是前后关系（也就是被观察顺序）。因此，上述三种情况分别是：</p>

<ul>
<li>A=1, B=2：thread 1 没有执行完X=10, Y=20的操作；thread 2 已经执行完了A=X, B=Y的操作。</li>
<li>A=10, B=2：thread 2 执行完X=10, 还没有执行Y=20；thread 2 已经执行完了A=X, B=Y的操作，所以Y的值还没有更新。</li>
<li>A=10, B=20：thread 1 执行完X=10, Y=20；thread 2读取了对应数据，X和Y都是最新的值。</li>
</ul>

<p>但是在CUDA使用的<strong>弱顺序内存模型</strong>中，可能会出现：</p>

<ul>
<li>A=1, B=20</li>
</ul>

<p>的情况。也就是说，虽然语义上A=10比B=20更早出现，但是CUDA的内存模型并不保证这一顺序。</p>

<p><strong>内存围栏函数可以被用来保证实际内存读取顺序和语义学顺序相同。</strong><br/>
在CUDA中提供了三个内存围栏函数：</p>

<pre><code class="language-cpp">void __threadfence_block();
void __threadfence();
void __threadfence_system();
</code></pre>

<p>这些函数都保证在函数调用前的内存操作不会晚于函数调用后的。</p>

<p>我们考虑一个比较常见的情景，这个技巧会在很多地方用到，大家也可以自己思考一下对这个问题有一些什么样的解决方案：<br/>
用一次核函数（kernel）调用计算N个数的和，也就是我们常说的reduction操作。我们实现的逻辑是：</p>

<ul>
<li>每个block首先计算这个数组的一个子集，并将计算结果写到全局内存中。提示，在改部分计算中可以使用warp原语、共享内存等方法来进一步加速，请参考相应的post。</li>
<li>当所有block都完成了这个计算，最后一个block从全局内存中读取这些部分和，并且将他们相加得到最终结果。</li>
<li>为了确认哪一个block最后完成操作。每一个block都会用原子加法(atomic add)将一个计数器（counter）加一。这样，最后一个block就是读取到counter值为gridDim.x-1的那个block。</li>
</ul>

<p>如果没有在存储部分和与计数器加一之间插入内存未来，计数器可能会在部分和存储完成之前加一，这样最终计算的结果可能会缺失一部分。</p>

<pre><code class="language-cpp">__device__ unsigned int count = 0; 
__shared__ bool isLastBlockDone; 

__global__ void sum(const float* array, unsigned int N, volatile float* result) 
{ 
    // Each block sums a subset of the input array. 
    float partialSum = calculatePartialSum(array, N); 
    
    if (threadIdx.x == 0) { 
        // Thread 0 of each block stores the partial sum 
        // to global memory. The compiler will use 
        // a store operation that bypasses the L1 cache 
        // since the &quot;result&quot; variable is declared as 
        // volatile. This ensures that the threads of 
        // the last block will read the correct partial 
        // sums computed by all other blocks. 
        result[blockIdx.x] = partialSum; 
        
        // Thread 0 makes sure that the incrementation 
        // of the &quot;count&quot; variable is only performed after 
        // the partial sum has been written to global memory. 
        __threadfence(); 
        
        // Thread 0 signals that it is done. 
        unsigned int value = atomicInc(&amp;count, gridDim.x); 
        
        // Thread 0 determines if its block is the last 
        // block to be done. 
        isLastBlockDone = (value == (gridDim.x - 1)); } 
        
        // Synchronize to make sure that each thread reads 
        // the correct value of isLastBlockDone. 
        __syncthreads(); 
        
        if (isLastBlockDone) { 
            // The last block sums the partial sums 
            // stored in result[0 .. gridDim.x-1] 
            float totalSum = calculateTotalSum(result); 
            
            if (threadIdx.x == 0) { 
                // Thread 0 of last block stores the total sum 
                // to global memory and resets the count 
                // varialble, so that the next kernel call 
                // works properly. 
                result[0] = totalSum; count = 0; 
            } 
        } 
    }
</code></pre>

<p>内存围栏只会影响一个线程中内存操作的顺序。他并不能像__syncthreads()那样保证这些内存操作对于同block中的其他线程可见。也就是说，由于有L1 cache的存在，其他block可能读取到的是没有更新的result值。在我们以往的经验中，这个是用__syncthreads()来保证的。在这个例子里，volatile限定词被用来保证内存操作的可见性。</p>

<h2 id="toc_1">volatile 限定词（Qualifier）</h2>

<p>从<a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#volatile-qualifier">CUDA编程手册</a>我们可以看到：</p>

<blockquote>
<p>The compiler is free to optimize reads and writes to global or shared memory (for example, by caching global reads into registers or L1 cache) as long as it respects the memory ordering semantics of memory fence functions (Memory Fence Functions) and memory visibility semantics of synchronization functions (Synchronization Functions).</p>

<p>These optimizations can be disabled using the volatile keyword: If a variable located in global or shared memory is declared as volatile, the compiler assumes that its value can be changed or used at any time by another thread and therefore any reference to this variable compiles to an actual memory read or write instruction.</p>
</blockquote>

<p>对于nvcc编译器来说，编译器是可以自由地优化全局内存(global memory)和共享内存(shared memory)的读写顺序。例如，这种优化可以是通过将共享内存的读取缓存到寄存器或者L1缓存中来实现的。但是，这种优化，必须要服从由内存围栏(memory fence)函数所定义出的内存顺序语义(memory ordering semantics)，还必须要服从同步函数(synchronization function)所限制的内存可见性语义（memory visibility semantics)。</p>

<p>如果我们对某一个内存（全局内存或共享内存）使用了volatile这个关键词，<strong>编译器将认为该内存的值允许在任意时刻被其他thread修改</strong>。因此，该变量的所有引用，都会被编译为真实的读写指令。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TensorFlow模型的保存，已经TensoRT中tensorFlow 模型的导入（草稿）]]></title>
    <link href="china.xipengli.com/20180121_TF_to_TRT.html"/>
    <updated>2018-01-21T16:08:07+08:00</updated>
    <id>china.xipengli.com/20180121_TF_to_TRT.html</id>
    <content type="html"><![CDATA[

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[深度学习中的优化调参细节（转帖）]]></title>
    <link href="china.xipengli.com/20180116_DL_Tuning_Reprint.html"/>
    <updated>2018-01-16T12:55:12+08:00</updated>
    <id>china.xipengli.com/20180116_DL_Tuning_Reprint.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">深度学习中的技巧</h2>

<ul>
<li>初始化参数尽量小一些，这样 softmax 的回归输出更加接近均匀分布，使得刚开始网络并不确信数据属于哪一类；另一方面从数值优化上看我们希望我们的参数具有一致的方差（一致的数量级），这样我们的梯度下降法下降也会更快。同时为了使每一层的激励值保持一定的方差，我们在初始化参数（不包括偏置项）的方差可以与输入神经元的平方根成反比。</li>
<li>学习率（learning rate）的设置应该随着迭代次数的增加而减小，个人比较喜欢每迭代完一次epoch也就是整个数据过一遍，然后对学习率进行变化，这样能够保证每个样本得到了公平的对待。</li>
<li>滑动平均模型，在训练的过程中不断的对参数求滑动平均这样能够更有效的保持稳定性，使其对当前参数更新不敏感。例如加动量项的随机梯度下降法就是在学习率上应用滑动平均模型。</li>
<li>在验证集上微小的提升未必可信，一个常用的准则是增加了30个以上的正确样本，能够比较确信算法有了一定的提升。 </li>
<li>不要太相信模型开始的学习速度，这与最终的结果基本没有什么关系。一个低的学习速率往往能得到较好的模型。</li>
<li>在深度学习中，常用的防止过拟合的方法除了正则化，dropout和pooling之外，还有提前停止训练的方法——就是看到我们在验证集的上的正确率开始下降就停止训练。</li>
<li>当激活函数是RELU时，我们在初始化偏置项时，为了避免过多的死亡节点（激活值为0）一般可以初始化为一个较小的正值。</li>
<li>基于随机梯度下降的改进优化算法有很多种，在不熟悉调参的情况，建议使用Adam方法</li>
<li>训练过程不仅要观察训练集和测试集的loss是否下降、正确率是否提高，对于参数以及激活值的分布情况也要及时观察，要有一定的波动。</li>
<li>如果我们设计的网络不work，在训练集的正确率也很低的话，我们可以减小样本数量同时去掉正则化项，然后进行调参，如果正确率还是不高的话，就说明我们设计的网络结果可能有问题。</li>
<li>fine-tuning的时候，可以把新加层的学习率调高，重用层的学习率可以设置的相对较低。</li>
<li>在隐藏层的激活函数，tanh往往比sigmoid表现更好。</li>
<li>针对梯度爆炸的情况我们可以使用梯度截断来解决，尤其在RNN中由于存在相同的循环结构，导致相同参数矩阵的连乘，更加容易产生梯度爆炸。当然，使用LSTM和GRU等更加优化的模型往往是更好地选择。</li>
<li>正则化输入，也就是让特征都保持在0均值和1方差。（注意做特征变换时请保持训练集合测试集进行了相同的变化）</li>
<li>梯度检验：当我们的算法在训练出现问题而进行debug时，可以考虑使用近似的数值梯度和计算的梯度作比较检验梯度是否计算正确。</li>
<li>搜索超参数时针对经典的网格搜索方法，这里有两点可以改善的地方：1）不用网格，用随机值，因为这样我们一次实验参数覆盖范围更广，尤其在参数对结果影响级别相差很大的情况下。2）不同数量级的搜索密度是不一样的，不能均分。</li>
</ul>

<h2 id="toc_1">CNN中的独特技巧</h2>

<ul>
<li>CNN中将一个大尺寸的卷积核可以分解为多层的小尺寸卷积核或者分成多层的一维卷积。这样能够减少参数增加非线性。</li>
<li>CNN中的网络设计应该是逐渐减小图像尺寸，同时增加通道数，让空间信息转化为高阶抽象的特征信息。</li>
<li>CNN中可以利用Inception方法来提取不同抽象程度的高阶特征，用ResNet的思想来加深网络的层数。</li>
<li>CNN处理图像时，常常会对原图进行旋转、裁剪、亮度、色度、饱和度等变化以增大数据集增加鲁棒性。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[TensorRT Plugin APIs 介绍（草稿）]]></title>
    <link href="china.xipengli.com/20180201_TRT_Plugin_API.html"/>
    <updated>2018-01-02T11:29:14+08:00</updated>
    <id>china.xipengli.com/20180201_TRT_Plugin_API.html</id>
    <content type="html"><![CDATA[
<p>TensorRT 中一个非常重要的特性是对于Plugin的支持。目前来说，标准Caffe中的CNN相关神经网络层已经完全支持。但是有很多改动版本的Caffe里面新添加的网络层是不支持的。例如SSD网络里面的priorbox, flatten, permute, detecion_out等。这些网络层在tensorRT里面可以使用Plugin的方式来支持。<br/>
因为整体上说，tensorRT的优化主要针对CBR (convolution-bias-relu)的合并，卷积的优化计算，低精度（int8，fp16）也主要是针对卷积层进行的。因此少量的plugin不会对性能产生较大影响。<br/>
但是对于YOLOv2这个网络，使用了目前tensorRT里面还不直接支持的PReLU, 如果把这个层使用plugin来实现，会出现CBR过程被打断，一些优化会受到影响，整体性能提高幅度可能不大。</p>

<p>本文先简要介绍一下定义一个plugin需要重载的IPlugin里面的函数。</p>

<p>总体上来说，TensorRT运行分为若干个步骤：</p>

<ol>
<li>创建网络(create network)，这一步可以是通过caffe 的parser，也就是以caffe的prototxt和caffemodel作为输入。nvCafferParser 回去分析这个网络结构，并且解析出其中各个网络层，如果是tensorRT已经支持的网络层，则会调用相应的tensorRT的API来添加到tensorRT的network中。如果是没有的网络层，但是已经在PluginFactory工厂里面定义了（通过isPlugin(layerName)来判断），则会标记为plugin添加到network中。</li>
<li>创建TensorRT引擎(create TensorRT engine)，这一步是tensorRT回去根据配置生成优化engine。主要的优化项目包括：合并网络层（例如CBR的合并，多个相同大小的CBR进行进一步合并），消除多余的网络层（例如concat层），根据提供的最大Batch数(maxium batch size)，workspace来选择相应的算法和CUDA实现(kernel)，预分配中间变量、输入输出buffer等。</li>
<li>运行时（runtime), 在这个阶段，</li>
</ol>

<h2 id="toc_0">When creating the network (from parser or API)</h2>

<ul>
<li>getNbOutputs()</li>
<li>getOutputDimensions()</li>
</ul>

<h2 id="toc_1">By the builder</h2>

<ul>
<li>configure()</li>
<li>getWorkSpaceSize()</li>
</ul>

<h2 id="toc_2">At runtime</h2>

<ul>
<li>initialize() when the engine context is constructed</li>
<li>enqueue() at inference time</li>
<li>terminate() when the engine context is destroyed</li>
</ul>

<h2 id="toc_3">For serialization</h2>

<ul>
<li>getSerializationSize()</li>
<li>serialize()</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[RNNs with equations]]></title>
    <link href="china.xipengli.com/15112406136583.html"/>
    <updated>2017-11-21T13:03:33+08:00</updated>
    <id>china.xipengli.com/15112406136583.html</id>
    <content type="html"><![CDATA[
<p>实际上使用公式来理解RNN更加简单，例如LSTM实际上就是求解了三个门i (input), o(output), f(forget), 三个门就是三个全连接层+激活层。而所谓的cell，就是计算输入的信息（本时刻，和上个时刻），至于这些信息是否保留，是两个门forget gate 和input gate来保证的。</p>

<h2 id="toc_0">LSTM</h2>

<ul>
<li>f: forget gate, 遗忘门，上一时刻的\(C_{t-1}\)的权重</li>
<li>i: input gate, 输入门，当前计算的\(C_{t}\)的权重</li>
<li>o: output gate, 输出门，当前计算的\(C_{t}\)到hidden output的权重。</li>
</ul>

<p>\[ f_t = \sigma(W_f\cdot [h_{t-1},x_t]^T+b_f)\]<br/>
\[ i_t = \sigma(W_i\cdot [h_{t-1},x_t]^T+b_i)\]<br/>
\[ \tilde{C}_t = \tanh (W_c\cdot [h_{t-1},x_t]^T+b_c)\]<br/>
\[ o_t = \sigma(W_o\cdot [h_{t-1},x_t]^T+b_o)\]<br/>
\[ C_t = f_t C_{t-1} + i_t  \tilde{C}_t \]<br/>
\[ h_t = o_t \tanh(C_t) \]</p>

<h2 id="toc_1">LSTM with peephole</h2>

<p>和LSTM相比，在计算各个gate的时候，顺便参考了一下\(C_{t-1}\)(input, forget)或者\(C_t\)(output)</p>

<p>\[ f_t = \sigma(W_f\cdot [C_{t-1},h_{t-1},x_t]^T+b_f)\]<br/>
\[ i_t = \sigma(W_i\cdot [C_{t-1},h_{t-1},x_t]^T+b_i)\]<br/>
\[ \tilde{C}_t = \tanh (W_c\cdot [h_{t-1},x_t]^T+b_c)\]<br/>
\[ C_t = f_t C_{t-1} + i_t  \tilde{C}_t \]<br/>
\[ o_t = \sigma(W_o\cdot [C_t,h_{t-1},x_t]^T+b_o)\]<br/>
\[ h_t = o_t \tanh(C_t) \]</p>

<h2 id="toc_2">LSTMP</h2>

<p>和LSTM相比，在计算最终的hidden output的时候，多添加了一个\(W_r\)<br/>
\[ f_t = \sigma(W_f\cdot [h_{t-1},x_t]^T+b_f)\]<br/>
\[ i_t = \sigma(W_i\cdot [h_{t-1},x_t]^T+b_i)\]<br/>
\[ \tilde{C}_t = \tanh (W_c\cdot [h_{t-1},x_t]^T+b_c)\]<br/>
\[ o_t = \sigma(W_o\cdot [h_{t-1},x_t]^T+b_o)\]<br/>
\[ C_t = f_t C_{t-1} + i_t  \tilde{C}_t \]<br/>
\[ h_t = W_r (o_t \tanh(C_t)) \]</p>

<h2 id="toc_3">GRU</h2>

<p>GRU相比LSTM更加简单，它只有两个门(z,r), 并且不需要单独的cell单元来保存状态。<br/>
\[ z_t = \sigma(W_z\cdot [h_{t-1}, x_t]^T)\]<br/>
\[ r_t = \sigma(W_r\cdot [h_{t-1}, x_t]^T)\]<br/>
\[ \tilde{h}_t = \tanh(W\cdot [r_t\times h_{t-1}, x_t]^T)\]<br/>
\[ h_t = (1-z_t)h_{t-1} + z_t \tilde{h}_t\]</p>

]]></content>
  </entry>
  
</feed>
