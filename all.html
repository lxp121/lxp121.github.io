<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  lxp121's BLOG on CUDA and DL
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="lxp121's BLOG on CUDA and DL" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:china.xipengli.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about.html">About</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; lxp121's BLOG on CUDA and DL</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
       
       <li><a href="index.html">HOME</a></li>
    <li><a href="archives.html">Archives</a></li>
    <li><a href="about.html">ABOUT</a></li>

    <li><label>Categories</label></li>

        
            <li><a href="dl.html">Deep Learning</a></li>
        
            <li><a href="cuda.html">CUDA</a></li>
        
            <li><a href="trt.html">TensorRT</a></li>
        
            <li><a href="Git.html">Git</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15231593087199.html">
                
                  <h1>TensorFlow-TensorRT 的使用及其限制（草稿）</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	
                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/4/8</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='dl.html'>Deep Learning</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15203065717420.html">
                
                  <h1>GPU代码的向后兼容问题</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>我们在开发GPU软件的时候，会遇到一个不同版本GPU支持的问题，GPU每一代都在不断的更新，新功能不断涌现，比如从Pascal GP102开始的INT8支持。但是我们很难为每一代产品去开发专门的kernel。这就存在一个代码向后兼容的问题。</p>

<p>在开始谈这个问题的时候我们来谈一些一个代码，或者准确说一个kernel被编译到可以在GPU上运行的。</p>

<h2 id="toc_0">nvcc 和 -gencode</h2>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/3/6</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='cuda.html'>CUDA</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180212_git_gitignore.html">
                
                  <h1>Git 修改.gitignore不起作用</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>.gitignore 只能忽略那些没有被track的文件，如果这些文件以及在版本管理里面了（通过git add)，则.gitignore是无效的。办法是删除现在的track状态（改变为未track），从新提交：</p>

<pre><code class="language-bash">git rm -r --cached .
git add .
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/2/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Git.html'>Git</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180212_git_move_commits_to_new_branch.html">
                
                  <h1>Git 将最近改动移到新的分支(branch)上</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">背景及 <em>错误</em> workflow</h2>

<p>我们有时候会发现针对某个项目做了大量改动，在push的时候发现自己并没有push权限。当然，这也不是正常的操作流程，正常的流程应该是：</p>

<pre><code class="language-flow">st=&gt;start: fork 一个新的分支
e=&gt;end: 推到服务器端
op=&gt;operation: 在新分支上做相应修改
op2=&gt;operation: 提交到新的服务器
op3=&gt;operation: 提交merge request
op4=&gt;operation: master 同意merge request，合并到现有branch中

st-&gt;op-&gt;op2-&gt;op3-&gt;op4-&gt;e
</code></pre>

<p>但是这样的操作有时候会被忘记，在已经提交了多个request之后，如何修改呢？<br/>
网上错误的教程是这样的：</p>

<pre><code class="language-bash"># WRONG workflow, not test!!
git branch -t newbranch
git reset --hard HEAD~3
git checkout newbranch
</code></pre>

<p>这样的后果是你下一次运行git rebase (或者git pull --rebase)的时候，这三次提交会消失。</p>

<h2 id="toc_1">正确的workflow</h2>

<pre><code class="language-bash">git reset --keep HEAD~3
git checkout -t -b newbranch
git cherry-pick ..HEAD@{2}
</code></pre>

<p>这三行代码进行了如下工作：</p>

<ul>
<li>首先它抛弃了最近三次提交。--keep 和 --hard很相似，但是更加安全一点，如果出现错误，不会丢弃没有提交的改动。</li>
<li>创建一个新的branch。</li>
<li>最后，使用cherry-picks 这个三个提交到新的分支 newbranch 上。因为这三个提交不再被任何一个分支引用，我们通过<em>relog</em>命令: <em>HEAD@{2}</em> 表示HEAD 两次操作(checkout 和reset)之前的提交。</li>
</ul>

<p>通常<em>reflog</em>是默认开启的，但是如果手动关闭了，则不能通过..HEAD@{2}找回三次之前的提交。</p>

<p>一个替代的方法是:</p>

<pre><code class="language-bash"># newbranch will omit the 3 most recent commits.
git checkout -b newbranch HEAD~3
git branch --set-upstream-to=oldbranch
# Cherry-picks the extra commits from oldbranch.
git cherry-pick ..oldbranch
# Discards the 3 most recent commits from oldbranch.
git branch --force oldbranch oldbranch~3
</code></pre>

<h2 id="toc_2">技术解释</h2>

<p>在上面的例子中，为什么 <em>git rebase</em> 会抛弃三次提交呢？这是因为<em>git rebase</em> 默认不会使用--fork-point 选项。</p>

<p>假设你有master分支包含M1，M2和M3三个提交。</p>

<pre><code>M1--M2--M3  &lt;-- origin/master
         \
          T1--T2--T3  &lt;-- topic
</code></pre>

<p>现在，如果有人修改了history，并且使用force-pushing 删除了M2</p>

<pre><code>M1--M3&#39;  &lt;-- origin/master
 \
  M2--M3--T1--T2--T3  &lt;-- topic
</code></pre>

<p>此时因为M3&#39;并不是分支topic里面的元素，因此git rebase 会认为T1-&gt;T3 已经不再需要（上游提交已经被删除）。</p>

<pre><code>M1--M3&#39;  &lt;-- origin/master
     \
      T1&#39;--T2&#39;--T3&#39;  &lt;-- topic (rebased)
</code></pre>

<p>这也是合理的操作。</p>

<p>因此在之前的例子中，</p>

<pre><code class="language-bash">git branch -t newbranch
git reset --hard HEAD~3
git checkout newbranch
</code></pre>

<p>Git将会把newbranch看做是从上游fork出来的一个分支，并且包含三个commits。然后<em>reset --hard</em> 会重写upstream的历史，并且删除三个提交。下一次你使用<em>git rebase</em>, 这三个提交会像其他从upstream里面删除的commit一样被抛弃掉。</p>

<p>但是对于我们这个情况，我们认为这三个commit应该是topic分支的一部分。 因此我们需要从upstream 更早的位置fork（不包含3个提交）。</p>

<h3 id="toc_3">注释</h3>

<p>因为大多数人不会去修改master branch, 因此即使错误的方法也没有很大的问题。</p>

<h3 id="toc_4">复现</h3>

<p>使用:</p>

<pre><code>touch M1 &amp;&amp; git add * &amp;&amp; git commit -m &quot;M1&quot;
touch M2 &amp;&amp; git add * &amp;&amp; git commit -m &quot;M2&quot;
touch M3 &amp;&amp; git add * &amp;&amp; git commit -m &quot;M3&quot;

git checkout -b topic
touch T1 &amp;&amp; git add * &amp;&amp; git commit -m &quot;T1&quot;
touch T2 &amp;&amp; git add * &amp;&amp; git commit -m &quot;T2&quot;
touch T3 &amp;&amp; git add * &amp;&amp; git commit -m &quot;T3&quot;

git checkout master
git reset HEAD~2
touch M3_primer &amp;&amp; rm M2 M3 &amp;&amp; git commit -m &quot;M3&#39;&quot;
git push --force

git checkout topic
git rebase master
</code></pre>

<ul>
<li>创建M1-&gt;M2-&gt;M3，此时master 包含文件: M1, M2, M3</li>
<li>创建topic 分支，此时topic包含：M1, M2, M3, T1, T2, T3</li>
<li>在master分支上reset到M1，并删除M2和M3两个文件，提交。</li>
<li>使用git push --froce提交到远端。</li>
<li>使用git checkout topic, git rebase master。会得到T3为M1, M3&#39;, T1, T2, T3。对于我们的情况是不对的（我们需要T1, T2, T3, M1, M2, M3)。但是逻辑上是正确的，因为你的base变了，会认为是新的T1&#39;，T2&#39;，T3&#39;。</li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/2/12</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='Git.html'>Git</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="20180125_CUDA_Warp_Level_Primitives.html">
                
                  <h1>使用 CUDA 的 warp-level 原语</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>本文是NVIDA Blog文章<a href="https://devblogs.nvidia.com/using-cuda-warp-level-primitives/">Using CUDA Warp-Level Primitives</a>的全文翻译和深度解析。</p>

<p>从CUDA 9.0 开始，CUDA引入了更加灵活的group的选择，这一方面使得CUDA编程更加简单，一方面也使得一些原有的功能发生了一些改变。本文重点对warp级别的原语（primitives）进行一些介绍。</p>

<p>在GPU中，线程（thread）被组织为warp，然后warp会按照SIMT（单指令多线程，Single Instruction Multiple Thread）的形式来执行。很多CUDA程序都可以通过充分利用warp执行来达到很高的性能。</p>

<h2 id="toc_0">Warp级别原语（Warp-level Primitives）</h2>

<p>NVIDIA GPUs和CUDA编程模型使用一种被称为SIMT的执行模式。SIMT和SIMD（Single Instruction, Multiple Data)的区别主要是：</p>

<ul>
<li>在SIMD架构中，每个指令在不同的数据上并行的进行相同的操作。SIMD通常采用向量寄存器和向量执行单元来实现。而vector执行是通过一个标量的向量来实现的。简言之，一个线程调度，多个向量寄存器和执行单元上并行完成相同指令。</li>
<li>而SIMT架构中，并不只是使用单一的线程，并且数据也不要求采用向量存储方式。多个线程会发起普通的指令（而非向量指令）在任意数据上执行。我们可以想象一下GPU的执行时候，可以根据每个thread的线程来计算需要操作的数据位置。实际上，可以在SIMT的架构中进一步实现SIMD的子架构，以Pascal架构中支持的INT8计算模式为例。该模式可以非常高效地提高深度神经网络的计算。在INT8的计算中，要求被计算的4个数据相互邻近（每个8bit），4个指令在一个GPU线程上执行。而这个线程是SIMT中的一个线程。</li>
</ul>

<p>在NVIDIA的GPU上，32个并行线程（相邻线程）被组成一个warp，每一个线程可以访问它自己的寄存器，从不同显存地址（可以不相邻）读写数据。并且，这些线程是可以支持分歧的控制流路径（divergent control flow paths)。本文中，我们重点放在使用Warp级别原语的使用。</p>

<p><em><a href="#code1">代码1</a></em> 是一个warp-level 原语的例子。我们使用了一个<strong>__shfl_down_sync()</strong> 来实现一个tree-reduction方式来计算一个warp中线程val值得和。该代码执行完之后，warp中的第一个线程中的val值就等于最终的和。</p>

<p><strong><div align = lift><span id="code1">代码1: warp-level reduction</span></div></strong> </p>

<pre><code class="language-cpp">#define FULL_MASK 0xffffffff
for (int offset = 16; offset &gt; 0; offset /= 2)
    val += __shfl_down_sync(FULL_MASK, val, offset);
</code></pre>

<p>这个计算可以用下面的图来说明：<br/>
<img src="media/15168532667210/15171231191469.png" alt="使用__shlf_down_sync()来实现warp级别的并行reduction"/><br/>
<strong><div align = center><span id="image1">图片1：使用__shlf_down_sync()来实现warp级别的并行reduction</span></div></strong></p>

<p>一个warp中包含32通道（lane），每一个线程占用一个通道。对于处于 X 通道的线程，<strong>__shlf_down_sync(FULL_MASK, val, offset)</strong> 来获得在同一个warp中的第 <strong>X+offset</strong> 通道的val值。这一数据交换在regitster中进行的，因此其效率高于共享内存。使用共享内存需要一次读，一次写，并且需要一个额外的寄存器来保存这些地址。</p>

<p>CUDA 9中包含了三类warp级别原语：</p>

<ol>
<li>同步地数据交换（Synchronized data exchange）：在同一warp中不同thread之间交换数据：

<ul>
<li>__all_sync, __any_sync, __uni_sync, __ballot_sync</li>
<li>__shfl_sync, __shfl_up_sync, __shfl_down_sync, __shfl_xor_sync</li>
<li>__match_any_sync, __match_all_sync</li>
</ul></li>
<li>Active mask query: 返回一个32-bit的mask来表示warp中哪些thread是活动的。

<ul>
<li>__activemask</li>
</ul></li>
<li>线程同步（Thread synchronization)：同步warp中的线程，并提供一个内存围栏（关于内存围栏的概念，可以在<a href="20180121_CUDA_Volatile_And_Memory_Fence.html">Volatile 限定词（Qualifier）和内存围栏（memory fence) </a>中找到详细解释）。</li>
</ol>

<h2 id="toc_1">同步地数据交换（Synchronized data exchange）</h2>

<p>每一个”同步地数据交换“原语都会在warp中的一组线程（不一定是所有线程）上执行一个聚合操作（collective operation)。<em><a href="#code2">代码2</a></em> 展示了其中的三个操作。每个线程可以调用 <strong>__shfl_sync()</strong> 或者 <strong>__shfl_down_sync()</strong> 来接收同一个warp中其他线程的数据。每个线程也可以调用<strong>__ballot_sync()</strong>来检验一个mask中的所有活动线程是否满足predicate的条件。该原语返回一个整形数，该整形数的第N位(bit)为真，当且仅当：</p>

<ul>
<li>predicate的检测结果为真；</li>
<li>第N个线程为active。</li>
</ul>

<p><strong><div align = lift><span id="code2">代码2: Synchronized data exchange warp-level primitives</span></div></strong> </p>

<pre><code class="language-cpp">int __shfl_sync(unsigned mask, int val, int src_line, int width=warpSize);
int __shfl_down_sync(unsigned mask, int var, unsigned detla, 
                     int width=warpSize);
int __ballot_sync(unsigned mask, int predicate);
</code></pre>

<p>通常使用一个32位的掩码（mask）来表示参与原语的线程。每一个参与的线程在执行相应操作之前必须同步，以保证其得到正确的结果。例如<strong>__shfl_sync</strong>实际是先同步所有的数据，再执行shfl的操作。因此<strong>__shfl_sync</strong>实际上可以保证一个同步。这里是一个很重要的变化：<strong>在CUDA9.0 之前，__shfl这个操作实际上是不保证一定同步的，但是在绝大多数的应用中，并没有出现不同步的情况。在CUDA9.0 中，不再使用__shfl等非同步原语。如果你有CUDA9.0之前的代码，使用了__shfl原语，请将其更换为__shfl_sync。</strong></p>

<p>一个常被问到的问题是：应该使用什么样的掩码参数？我们可以简单的理解为mask就是一组需要参与聚合操作的线程的id。所以，这个线程组的组成实际是程序逻辑决定的。我们还是以<em><a href="#code1">代码1</a></em> 这个reduction代码为例子。假设我们希望计算input[]数组所有元素的和，并且这个数组长度NUM_ELEMENTS是小于这个block里面的线程数目的。我们可以使用<em><a href="#code3">代码3</a></em>这个代码来实现。 </p>

<p><strong><div align = lift><span id="code3">代码3: </span></div></strong> </p>

<pre><code class="language-cpp">unsigned mask = __ballot_sync(FULL_MASK, threadIdx.x &lt; NUM_ELEMENTS);
if (threadIdx.x &lt; NUM_ELEMENTS) { 
    val = input[threadIdx.x]; 
    for (int offset = 16; offset &gt; 0; offset /= 2)
        val += __shfl_down_sync(mask, val, offset);
    …
}
</code></pre>

<p>这个代码首先使用 <em>threadIdx.x &lt; NUM_ELEMENTS</em> 来确认一个线程是否参与这个reduction过程。其中<em>__ballot_sync()</em> 是用来计算<em>__shlf_down_sync()</em>中使用的掩码的。而<em>__ballot_sync()</em>本身使用FULL_MASK （0xffffffff），表示所有的32个线程都需要参与投票。</p>

<p>在Volta或者更晚一些的架构中，数据交换原语可以被用于线程分歧分支（thread-divergent branchs）中-- 一个线程可以执行和warp中其他线程不同的路径。<em><a href="#code4">代码4</a></em> 展示了这样一个例子，一个warp中所有线程获得第0通道的val值，但是奇数和偶数的线程执行一个if语句中的不同分支。 </p>

<p><strong><div align = lift><span id="code4">代码4: </span></div></strong> </p>

<pre><code class="language-cpp">if (threadIdx.x % 2) {
    val += __shfl_sync(FULL_MASK, val, 0);
…
}
else {
    val += __shfl_sync(FULL_MASK, val, 0);
…
}
</code></pre>

<p>在最新的Volta（或者未来的）GPU上，你可以使用warp同步原语，而不需要去考虑这个函数是执行在哪个线程分歧分支上。</p>

<h2 id="toc_2">Active Mask Query</h2>

<p>__activemask() returns a 32-bit unsigned int mask of all currently active threads in the calling warp. In other words, it shows the calling thread which threads in its warp are also executing the same __activemask(). This is useful for the :opportunistic warp-level programming” technique we explain later, as well as for debugging and understanding program behavior.</p>

<p>However, it’s important to use __active_mask() correctly. Listing 5 illustrates an incorrect use. The code tries to perform the same sum reduction  shown in Listing 4, but instead of using __ballot_sync() to compute the mask before the branch, it uses __active_mask() inside the branch. This is incorrect, as it would result in partial sums instead of a total sum. The CUDA execution model does not guarantee that all threads taking the branch together will execute the __active_mask() together. Implicit lock step execution is not guaranteed, as we will explain.</p>

<p><strong><div align = lift><span id="code5">代码5: </span></div></strong> </p>

<pre><code class="language-cpp">//
// Incorrect use of __active_mask()
//
if (threadIdx.x &lt; NUM_ELEMENTS) { 
    unsigned mask = __active_mask(); 
    val = input[threadIdx.x]; 
    for (int offset = 16; offset &gt; 0; offset /= 2)
        val += __shfl_down_sync(mask, val, offset);
    …
}
</code></pre>

<h2 id="toc_3">Warp 同步（Synchronization）</h2>

<p>When threads in a warp need to perform more complicated communications or collective operations than what the data exchange primitives provide, you can use the __syncwarp() primitive to synchronize threads in a warp. It is similar to the __syncthreads() primitive (which synchronizes all threads in the thread block) but at finer granularity.</p>

<pre><code class="language-cpp">void __syncwarp(unsigned mask=FULL_MASK);
</code></pre>

<p>The __syncwarp() primitive causes the executing thread to wait until all threads specified in mask have executed a __syncwarp() (with the same mask) before resuming execution. It also provides a memory fence to allow threads to communicate via memory before and after calling the primitive.</p>

<p>Listing 6 shows an example of shuffling the ownership of matrix elements among threads in a warp.</p>

<pre><code class="language-cpp">float val = get_value(…);
__shared__ float smem[4][8];
 
//   0  1  2  3  4  5  6  7 
//   8  9 10 11 12 13 14 15 
//  16 17 18 19 20 21 22 23
//  24 25 26 27 28 29 30 31
int x1 = threadIdx.x % 8;
int y1 = threadIdx.x / 8;
 
//   0  4  8 12 16 20 24 28
//   1  5 10 13 17 21 25 29
//   2  6 11 14 18 22 26 30
//   3  7 12 15 19 23 27 31
int x2= threadIdx.x / 4;
int y2 = threadIdx.x % 4;
 
smem[y1][x1] = val;
__syncwarp();
val = smem[y2][x2];
 
use(val);
</code></pre>

<p>Assume a 1-D thread block is used (i.e. threadIdx.y is always 0). At the beginning of the code, each thread in a warp owns one element of a 4×8 matrix with row-major indexing. In other words, lane 0 owns [0][0] and lane 1 owns [0][1]. Each thread stores its value into the corresponding position of a 4×8 array in shared memory. Then __syncwarp() is used to ensure all threads have done the store, before each thread reads from a transposed position in the array. In the end, each thread in the warp owns one element of the matrix with column-major indexing: lane 0 owns [0][0] and lane 1 owns [1][0].</p>

<p>Make sure that __syncwarp() separates shared memory reads and writes to avoid race conditions. Listing 7 illustrates an incorrect use in a tree sum reduction in shared memory. There is a shared memory read followed by a shared memory write between every two __syncwarp() calls. The CUDA programming model does not guarantee that all the reads will be performed before all the writes, so there is a race condition.</p>

<pre><code class="language-cpp">unsigned tid = threadIdx.x;

// Incorrect use of __syncwarp()
shmem[tid] += shmem[tid+16]; __syncwarp();
shmem[tid] += shmem[tid+8];  __syncwarp();
shmem[tid] += shmem[tid+4];  __syncwarp();
shmem[tid] += shmem[tid+2];  __syncwarp();
shmem[tid] += shmem[tid+1];  __syncwarp();
</code></pre>

<p>Listing 8 fixes the race condition by inserting extra __syncwarp() calls. The CUDA compiler may elide some of these synchronization instructions in the final generated code depending on the target architecture (e.g. on pre-Volta architectures).</p>

<pre><code class="language-cpp">unsigned tid = threadIdx.x;
int v = 0;

v += shmem[tid+16]; __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+8];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+4];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+2];  __syncwarp();
shmem[tid] = v;     __syncwarp();
v += shmem[tid+1];  __syncwarp();
shmem[tid] = v;
</code></pre>

<p>On the latest Volta (and future) GPUs, you can also use __syncwarp() in thread-divergent branches to synchronize threads from both branches. But once they return from the primitive, the threads will become divergent again. See Listing 13 for such an example.</p>

<h2 id="toc_4">Opportunistic Warp-level Programming</h2>

<p>As we showed in the Synchronized Data Exchange section, the membership mask used in the synchronized data exchange primitives is often computed before a branch condition in the program flow. In many cases, the program needs to pass the mask along the program flow; for example, as a function argument when warp-level primitives are used inside a function. This may be difficult if you want to use warp-level programming inside a library function but you cannot change the function interface.</p>

<p>Some computations can use whatever threads happen to be executing together. We can use a technique called opportunistic warp-level programming, as the following example illustrates. (See this post on warp-aggregated atomics for more information on the algorithm, and this post for discussion of how Cooperative Groups makes the implementation much simpler.)</p>

<pre><code class="language-cpp">// increment the value at ptr by 1 and return the old value
__device__ int atomicAggInc(int *ptr) {
    int pred;
    int mask = __match_all_sync(__activemask(), ptr, &amp;pred);
    int leader = __ffs(mask) – 1;    // select a leader
    int res;
    if(lane_id() == leader)                  // leader does the update
        res = atomicAdd(ptr, __popc(mask));
    res = __shfl_sync(mask, res, leader);    // get leader’s old value
    return res + __popc(mask &amp; ((1 &lt;&lt; lane_id()) – 1)); //compute old value
}
</code></pre>

<p>atomicAggInc() atomically increments the value pointed to by ptr by 1 and returns the old value. It uses the atomicAdd() function, which may incur contention. To reduce contention, atomicAggInc replaces the per-thread atomicAdd() operation with a per-warp atomicAdd(). The __activemask() in line 4 finds the set of threads in the warp that are about to perform the atomic operation. __match_all_sync() returns the bit mask of the threads that have the same value ptr, partitioning the incoming threads into groups whose members have the same ptr value. Each group elects a leader thread (line 5), which performs the atomicAdd() (line 8) for the whole group. Every thread gets the old value from the leader (line 9) returned by the atomicAdd(). Line 10 computes and returns the old value the current thread would get from atomicInc() if it were to call the function instead of atomicAggInc.</p>

<h2 id="toc_5">Implicit Warp-Synchronous Programming is Unsafe</h2>

<p>CUDA toolkits prior to version 9.0 provided a (now legacy) version of warp-level primitives. Compared with the CUDA 9 primitives, the legacy primitives do not accept a mask argument. For example, int __any(int predicate) is the legacy version of int __any_sync(unsigned mask, int predicate).</p>

<p>The mask argument, as explained previously, specifies the set of threads in a warp that must participate in the primitives. The new primitives perform intra-warp thread-level synchronization if the threads specified by the mask are not already synchronized during execution.</p>

<p>The legacy warp-level primitives do not allow programmers to specify the required threads and do not perform synchronization. Therefore, the threads that must participate in the warp-level operation are not explicitly expressed by the CUDA program. The correctness of such a program depends on implicit warp-synchronous behavior, which may change from one hardware architecture to another, from one CUDA toolkit release to another (due to changes in compiler optimizations, for example), or even from one run-time execution to another. Such implicit warp-synchronous programming is unsafe and may not work correctly.</p>

<p>For example, in the following code, let’s assume all 32 threads in a warp execute line 2 together. The if statement at line 4 causes the threads to diverge, with the odd threads calling foo() at line 5 and the even threads calling bar() at line 8.</p>

<pre><code class="language-cpp">// Assuming all 32 threads in a warp execute line 1 together.
assert(__ballot(1) == FULL_MASK);
int result;
if (thread_id % 2) {
    result = foo();
}
else {
    result = bar();
}
unsigned ballot_result = __ballot(result);
</code></pre>

<p>The CUDA compiler and the hardware will try to re-converge the threads at line 10 for better performance. But this re-convergence is not guaranteed. Therefore, the ballot_result may not contain the ballot result from all 32 threads.</p>

<p>Calling the new __syncwarp() primitive at line 10 before __ballot(), as illustrated in Listing 11, does not fix the problem either. This is again implicit warp-synchronous programming. It assumes that threads in the same warp that are once synchronized will stay synchronized until the next thread-divergent branch. Although it is often true, it is not guaranteed in the CUDA programming model.</p>

<pre><code class="language-cpp">__syncwarp();
unsigned ballot_result = __ballot(result);
</code></pre>

<p>The correct fix is to use __ballot_sync() as in Listing 12.</p>

<pre><code class="language-cpp">unsigned ballot_result = __ballot_sync(FULL_MASK, result);
</code></pre>

<p>A common mistake is to assume that calling __syncwarp() before and/or after a legacy warp-level primitive is functionally equivalent to calling the sync version of the primitive. For example, is __syncwarp(); v = __shfl(0); __syncwarp(); the same as __shfl_sync(FULL_MASK, 0)? The answer is no, for two reasons. First, if the sequence is used in a thread-divergent branch, then __shfl(0) won’t be executed by all threads together. Listing 13 shows an example. The __syncwarp() at line 3 and line 7 would ensure foo() is called by all threads in the warp before line 4 or line 8 is executed. Once threads leave the __syncwarp(), the odd threads and the even threads become divergent again. Therefore, the __shfl(0) at line 4 will get an undefined value because lane 0 is inactive when line 4 is executed. __shfl_sync(FULL_MASK, 0) can be used in thread-divergent branches without this problem.</p>

<pre><code class="language-cpp">v = foo();
if (threadIdx.x % 2) {
    __syncwarp();
    v = __shfl(0);       // L3 will get undefined result because lane 0 
    __syncwarp();        // is not active when L3 is executed. L3 and L6
} else {                 // will execute divergently.
    __syncwarp();
    v = __shfl(0);
    __syncwarp();
}
</code></pre>

<p>Second, even when the sequence is called by all the threads together, the CUDA execution model does not guarantee threads will stay convergent after leaving __syncwarp(), as Listing 14 shows. Implicit lock-step execution is not guaranteed. Remember, thread convergence is guaranteed only within explicitly synchronous warp-level primitives.</p>

<pre><code class="language-cpp">assert(__activemask() == FULL_MASK); // assume this is true
__syncwarp();
assert(__activemask() == FULL_MASK); // this may fail
</code></pre>

<p>Because using them can lead to unsafe programs, the legacy warp-level primitives are deprecated starting in CUDA 9.0.</p>

<h2 id="toc_6">Update Legacy Warp-Level Programming</h2>

<p>If your program uses legacy warp-level primitives or any form of implicit warp-synchronous programming (such as communicating between threads of a warp without synchronization), you should  update the code to use the sync version of the primitives. You may also want to restructure your code to use Cooperative Groups, which provides a higher level of abstraction as well as new features such as multi-block synchronization.</p>

<p>The trickiest part of using the warp-level primitives is figuring out the membership mask to be used. We hope the above sections give you a good idea where to start and what to look out for.  Here is a list of suggestions:</p>

<ol>
<li>Don’t just use FULL_MASK (i.e. 0xffffffff for 32 threads) as the mask value. If not all threads in the warp can reach the primitive according to the program logic, then using FULL_MASK may cause the program to hang.</li>
<li>Don’t just use __activemask() as the mask value. __activemask() tells you what threads happen to be convergent when the function is called, which can be different from what you want to be in the collective operation.</li>
<li>Do analyze the program logic and understand the membership requirements. Compute the mask ahead based on your program logic.</li>
<li>If your program does opportunistic warp-synchronous programming, use “detective” functions such as __activemask() and __match_all_sync() to find the right mask.</li>
<li>Use __syncwarp() to separate operations with intra-warp dependences. Do not assume lock-step execution.</li>
</ol>

<p>One last trick. If your existing CUDA program gives a different result on Volta architecture GPUs, and you suspect the difference is caused by Volta’s new independent thread scheduling which can change warp synchronous behavior, you may want to recompile your program with nvcc options -arch=compute_60 -code=sm_70. Such compiled programs opt-in to Pascal’s thread scheduling. When used selectively, it can help pin down the culprit module more quickly, allowing you to update the code to avoid implicit warp-synchronous programming.</p>

<p><img src="media/15168532667210/15171479419797.png" alt=""/><br/>
<strong><div align = center><span id="image2">图片2：Volta independent thread scheduling enables interleaved execution of statements from divergent branches. This enables execution of fine-grain parallel algorithms where threads within a warp may synchronize and communicate.</span></div></strong></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2018/1/25</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='cuda.html'>CUDA</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			 <a href="all_1.html">&raquo; Next Page</a> 
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="media/15161603048345/Blog_LOGO.png" /></div>
            
                <h1>lxp121's BLOG on CUDA and DL</h1>
                <div class="site-des"></div>
                <div class="social">



<a target="_blank" class="linkedin" href="https://www.linkedin.com/in/xipengli" title="LinkedIn">LinkedIn</a>





<a target="_blank" class="github" target="_blank" href="https://github.com/lxp121" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:lxp121@gmail.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="dl.html"><strong>Deep Learning</strong></a>
        
            <a href="cuda.html"><strong>CUDA</strong></a>
        
            <a href="trt.html"><strong>TensorRT</strong></a>
        
            <a href="Git.html"><strong>Git</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15231593087199.html">TensorFlow-TensorRT 的使用及其限制（草稿）</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15203065717420.html">GPU代码的向后兼容问题</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180212_git_gitignore.html">Git 修改.gitignore不起作用</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180212_git_move_commits_to_new_branch.html">Git 将最近改动移到新的分支(branch)上</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="20180125_CUDA_Warp_Level_Primitives.html">使用 CUDA 的 warp-level 原语</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    <script src="asset/chart/all-min.js"></script><script type="text/javascript">$(function(){    var mwebii=0;    var mwebChartEleId = 'mweb-chart-ele-';    $('pre>code').each(function(){        mwebii++;        var eleiid = mwebChartEleId+mwebii;        if($(this).hasClass('language-sequence')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = Diagram.parse($(this).text());            diagram.drawSVG(eleiid,{theme: 'simple'});        }else if($(this).hasClass('language-flow')){            var ele = $(this).addClass('nohighlight').parent();            $('<div id="'+eleiid+'"></div>').insertAfter(ele);            ele.hide();            var diagram = flowchart.parse($(this).text());            diagram.drawSVG(eleiid);        }    });});</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
